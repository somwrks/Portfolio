{
  "projects": [
    {
      "title": "SparkyAI",
      "description": "RAG based GenAI Multi Agent architecture bot  to automate discord server",
      "readme": "# ASU Discord Bot - (Public)\n\nThis repository has been recently made public, the old repository had exposed credential issues.\n\nA Discord bot designed to assist Arizona State University (ASU) students with access to resources, including news, events, scholarships, courses, and more.\n\n![{2B61349D-750C-4418-A76E-15CB3AAB0B8B}](https://github.com/user-attachments/assets/642fd6d6-5232-4347-b1dc-3e78d3d0c758)\n\n![{5B0799E0-D15C-4017-867A-F1DEB1FDA2DC}](https://github.com/user-attachments/assets/e19a175a-2c70-4af8-88d8-6303b9729cda)\n\n![{2DDB8F4F-5F0E-4828-8FDD-847E67C40A65}](https://github.com/user-attachments/assets/7fbce508-e180-4f8f-9d7f-11feac5757e8)\n\n## Advanced Architecture\n\nSparkyAI leverages a complex architecture to deliver accurate and context-aware responses:\n\n- **Retrieval-Augmented Generation (RAG)**: Combines vector search with large language models for precise information retrieval.\n- **Multi-Agent System**: Utilizes specialized AI agents (Action, Google, Search, Live Status, Discord) for targeted task execution.\n- **Vector Database**: Implements Qdrant for efficient semantic search and document retrieval.\n\n## Advanced Features\n\n- **Maximum Inner Product Search (MIPS)**: Optimized vector similarity search for large-scale datasets.\n- **RAPTOR Retrieval**: Hierarchical document representation for nuanced information retrieval.\n- **Multi-step Reasoning**: Synthesizes information from multiple sources to answer complex queries.\n- **Dynamic Content Extraction**: Combines Selenium-based scraping with AI-powered content refinement.\n- **Automatic Version Control**: Intelligent document updating based on timestamp comparisons.\n- **Cross-Encoder Reranking**: Implements a cross-encoder model to rerank initial retrieval results, improving the relevance of top results.\n- **HNSWlib Integration**: Incorporates a graph-based approach for efficient in-memory searches, complementing existing vector search methods.\n- **ScaNN Integration**: Utilizes Google's ScaNN library for scalable and efficient handling of large-scale, high-dimensional vector datasets.\n- **Custom Embedding Model**: Utilizes the BAAI/bge-large-en-v1.5 model for high-quality text embeddings.\n- **Cross-Encoder Reranking**: Implements the cross-encoder/ms-marco-MiniLM-L-6-v2 model to rerank initial retrieval results, improving the relevance of top results\n\n## Performance Optimizations\n\n- **Batch Processing**: Efficient document storage and retrieval in configurable batches.\n- **Caching Mechanisms**: Implements strategic caching for frequently accessed data.\n- **Asynchronous Operations**: Utilizes asyncio for non-blocking I/O operations.\n- **Retry Mechanisms**: Robust error handling with configurable retry attempts for critical operations.\n- **Multi-Method Search**: Combines RAPTOR, similarity search, MIPS, and ScaNN for comprehensive and efficient information retrieval.\n- **Result Deduplication**: Implements intelligent merging and deduplication of search results from multiple methods.\n\n## Key Features\n\nThe code implements the following key features:\n\n### Web Scraping & Summarization\n\nThe `ASUWebScraper` class handles web scraping of ASU pages:\n\n- Uses Selenium for dynamic content and BeautifulSoup for static HTML parsing\n- Implements methods for scraping specific ASU resources like course catalogs, library resources, and job postings\n- Extracts content using both Selenium-based scraping and Jina AI-powered content extraction\n\nThe `DataPreprocessor` class handles summarization:\n\n- Cleans and structures scraped text using NLP techniques like tokenization and lemmatization\n- Uses AI agents (likely the Gemini model) to refine and summarize content\n\n### Discord Integration\n\nThe `DiscordState` class manages the Discord bot's state and interactions:\n\n- Initializes Discord intents for message content and member access\n- Tracks user information, roles, and channel details\n- Provides methods to update and retrieve bot state\n\n### AI-Driven Information Retrieval\n\nThe system uses a Retrieval-Augmented Generation (RAG) architecture:\n\n- The `VectorStore` class manages document storage and retrieval using Qdrant vector database\n- Implements semantic search capabilities using HuggingFace embeddings\n- The `Utils` class contains methods for similarity search and database querying\n\n### Utils Class Enhancements\n\n- **Multi-Method Search**: Orchestrates searches across RAPTOR, similarity, MIPS, and ScaNN methods.\n- **Result Merging**: Intelligently combines and deduplicates results from various search methods.\n- **Ground Source Management**: Tracks and manages unique source URLs for comprehensive information retrieval.\n- **Caching**: Implements query and document ID caching for improved performance.\n\n### Webhooks\n\nThe system uses various sets of custom web scraping functions with search query manipulation to fetch most results-\n\n- The `ASUWebScraper` class can be extended to fetch data from different ASU platforms\n\n## Database Integration\n\nThe ASU Discord Bot utilizes two database systems for different purposes:\n\n### Vector Store Operations\n\n- **Qdrant Integration**: Utilizes Qdrant for efficient vector storage and retrieval.\n- **MIPS Search**: Implements Maximum Inner Product Search for optimized similarity queries.\n- **HNSWlib Indexing**: Builds and uses HNSW indexes for fast approximate nearest neighbor search.\n- **Automatic Index Building**: Dynamically constructs search indexes for improved query performance.\n\n### Google Sheets Database\n\nThe `GoogleSheet` class manages interactions with a Google Sheets database, primarily used for moderator oversight and user tracking.\n\nKey features:\n\n- User Management: Stores and retrieves user information, including Discord IDs, names, and email addresses.\n- Function Call Tracking: Increments counters for various bot function calls, allowing moderators to monitor usage patterns.\n- Data Retrieval: Provides methods to fetch all users or specific user data.\n- Data Updates: Allows updating user-specific information in the spreadsheet.\n\nImplementation details:\n\n- Uses the Google Sheets API for read and write operations.\n- Implements methods like `get_all_users()`, `add_new_user()`, and `update_user_column()`.\n- Provides error handling and logging for database operations.\n\n### Firestore Database\n\nThe `Firestore` class manages interactions with Google's Firestore, used for storing bot-related data and chat histories.\n\nKey features:\n\n- Chat History: Stores complete conversation histories between users and the bot.\n- Message Categorization: Organizes messages by different agent types (action, discord, google, live status, search).\n- Real-time Updates: Leverages Firestore's real-time capabilities for instant data synchronization.\n\nImplementation details:\n\n- Initializes a Firestore client using Firebase Admin SDK.\n- Implements methods to update collections, add new messages, and retrieve chat histories.\n- Stores messages with timestamps and user IDs for comprehensive tracking.\n\n### Finetune\n\nWe are finetuning gemini 1.5 flash model (Action Agent) with our custom dataset containing 560 examples of different interactions between agents with humans aswell as agents with other agents to increase the accuracy of reasoning aswell as general responses to students:\n\n| **Category**               | **Description**                                                                                 | **Proportion (%)** | **Number of Examples** |\n| -------------------------------- | ----------------------------------------------------------------------------------------------------- | ------------------------ | ---------------------------- |\n| **Factual Questions**      | Questions requiring concise, factual answers, such as \"What are the library hours?\"                   | 23.6%                    | 130                          |\n| **Action-Based Questions** | Queries requiring JSON function calls, such as \"Find the latest scholarships.\"                        | 32.7%                    | 180                          |\n| **Hybrid Questions**       | Queries needing reasoning + a function call, such as \"Can you summarize this and get related events?\" | 32.7%                    | 180                          |\n| **Jailbreak Commands**     | Edge-case inputs requiring safe acknowledgment or refusal                                             | 10.9%                    | 60                           |\n\n### Extensible Design\n\nThe modular architecture allows for easy extension:\n\n- Multiple agent classes (e.g., `ActionAgent`, `SearchAgent`, `GoogleAgent`) can be customized for different tasks\n- The `AppConfig` class centralizes configuration management, making it easy to add new features\n- The use of asynchronous programming (async/await) throughout the codebase allows for efficient handling of concurrent operations\n\n## Technologies Used\n\n- **AI/ML**: Gemini Vertex AI, LangChain, TensorFlow\n- **NLP**: Hugging Face Transformers, NLTK\n- **Embeddings**: BAAI/bge-large-en-v1.5 model\n- **Cross Encoder**:  cross-encoder/ms-marco-MiniLM-L-6-v2\n- **Vector Search**: Qdrant, FAISS\n- **Web Scraping**: Selenium, BeautifulSoup4\n- **APIs**: Discord API, Google Sheets API, Google Cloud Storage\n- **Databases**: Firestore, Google Sheets (for moderation)\n- **Asynchronous Programming**: asyncio\n- **Containerization**: Docker\n\n## Agent Descriptions\n\n| Name                        | Importance                                           | What It Does                                                                                                              | Functions                                                                                                                                                                                                                                                                            |\n| --------------------------- | ---------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| **Action Agent**      | Central coordinator for user interactions            | Handles main messages, decides on direct responses or function calls, and utilizes multiple agents/functions as needed.   | -`access_search_agent<br>`- `access_discord_agent<br>`- `access_google_agent<br>`- `access_live_status_agent<br>`- `get_discord_server_info<br>`- `get_user_profile_details`                                                                                             |\n| **Google Agent**      | Specialized search functionality                     | Performs Google searches for queries; defers to Action Agent for complex queries requiring the Search Agent.              | -`google_search_tool`                                                                                                                                                                                                                                                              |\n| **Search Agent**      | Executes functions for queries from the Action Agent | Executes specific functions to retrieve information based on queries passed by the Action Agent; responds in JSON format. | -`get_latest_club_information<br>`- `get_latest_event_updates<br>`- `get_latest_news_updates<br>`- `get_latest_social_media_updates<br>`- `get_latest_sport_updates<br>`- `get_library_resources<br>`- `get_latest_scholarships<br>`- `get_latest_class_information` |\n| **Live Status Agent** | Manages live status-related queries                  | Executes live status functions for queries from the Action Agent; responds in JSON format.                                | -`get_live_library_status<br>`- `get_live_shuttle_status`                                                                                                                                                                                                                        |\n| **Discord Agent**     | Handles Discord-specific functionalities             | Executes Discord-related functions for queries from the Action Agent; responds in JSON format.                            | -`notify_discord_helpers<br>`- `notify_moderators` `<br>`- `create_discord_forum_post<br>`- `create_discord_announcement<br>`- `create_discord_event<br>`- `create_discord_poll`                                                                                       |\n\n## Getting Started\n\n1. Clone the repository:\n\n   ```bash\n   git clone https://github.com/somwrks/SparkyAI.git\n   cd sparkyai\n   ```\n2. Install dependencies:\n\n   ```bash\n   pip install -r requirements.txt\n   ```\n3. Add API keys:\n\n   - Create `config.json` with keys for Google Search, Discord, and ASU webhooks.\n   - Add `clientsecret.json` for GoogleSheet API.\n4. Run the bot:\n\n   ```bash\n   python main.py\n   ```\n\n## Contributing\n\nContributions are welcome! Please open an issue or submit a pull request for any ideas or improvements.\n\n## Citations\n\nThis project draws inspiration from and builds upon the following research papers:\n\n1. [Amagata, D., &amp; Hara, T. (2023). Reverse Maximum Inner Product Search: Formulation, Algorithms, and Analysis. ACM Transactions on the Web, 17(4), 1-23](https://dl.acm.org/doi/pdf/10.1145/3587215)\n2. [Sun, P. (2020). Announcing ScaNN: Efficient Vector Similarity Search. Google Research Blog](https://research.google/blog/announcing-scann-efficient-vector-similarity-search/)\n3. [Guo, R., Sun, P., Lindgren, E., Geng, Q., Simcha, D., Chern, F., &amp; Kumar, S. (2020). Accelerating Large-Scale Inference with Anisotropic Vector Quantization. International Conference on Machine Learning (ICML)](https://arxiv.org/pdf/1908.10396)\n4. [Dong, W., Moses, C., &amp; Li, K. (2024). SOAR: Improved Indexing for Approximate Nearest Neighbor Search. arXiv preprint arXiv:2404.00774](https://www.arxiv.org/pdf/2411.06158)\n5. [Kandpal, N., Jiang, H., Kong, X., Teng, J., &amp; Chen, J. (2024). RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval. arXiv preprint arXiv:2401.18059v1](https://arxiv.org/pdf/2401.18059v1)\n6. [Guo, R., Kumar, S., Choromanski, K., &amp; Simcha, D. (2019). Quantization based Fast Inner Product Search. arXiv preprint arXiv:1509.01469](https://arxiv.org/pdf/1509.01469)\n\nThese papers have significantly contributed to the field of vector similarity search, maximum inner product search (MIPS), and efficient indexing techniques, which are fundamental to this project's approach.\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n\n## Contact\n\n- Author: Ash (Som)\n- Portfolio: [somwrks.com](https://somwrks.com)\n- LinkedIn: [linkedin.com/somwrks](https://linkedin.com/somwrks)\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/SparkyAI",
      "commitCount": 43,
      "latestCommitDate": "2025-01-10T22:59:12Z",
      "topics": [
        "artificial-intelligence",
        "bot-development"
      ]
    },
    {
      "title": "AI-Solar-Panel",
      "description": "Self directional solar panel",
      "readme": "# AI Solar Panel\r\n\r\nAn innovative self-adjusting solar panel system that maximizes energy absorption using computer vision and deep learning.\r\n\r\n## Project Overview\r\n\r\nThis repository contains the deep learning component of our AI-powered solar panel tracking system. The project aims to optimize solar energy capture by accurately tracking the sun's position in real-time, using a lightweight deep learning model running on a microcontroller.\r\n\r\n## Key Features\r\n\r\n- **Sun Position Detection**: Utilizes a lightweight deep learning model for real-time sun tracking in various weather conditions.\r\n- **Microcontroller Integration**: Designed to run efficiently on a Teensy or Arduino microcontroller.\r\n- **Automated Adjustment**: Provides coordinates to guide the solar panel's orientation for maximum sunlight exposure.\r\n- **Fallback Algorithm**: Implements a non-ML algorithm for tracking during heavy cloud cover or rain.\r\n- **Performance Tracking**: Monitors wattage generation and usage for comparison between algorithmic and deep-learning modes.\r\n- **Adaptive Detection Intervals**: Adjusts detection frequency based on sun movement to optimize power usage.\r\n- **Camera Management**: Activates the camera only when needed for detection to conserve energy.\r\n\r\n## System Architecture\r\n\r\n1. 480p-720p camera captures sky images\r\n2. Deep learning model detects sun's position\r\n3. Microcontroller processes input and calculates optimal panel alignment\r\n4. Servo motors adjust panel's azimuth and attitude\r\n\r\n## Technical Details\r\n\r\n- **Model**: Lightweight deep learning model (YOLOv8 converted via LiteRT)\r\n- **Hardware**: Teensy or Arduino Microcontroller\r\n- **Additional Sensors**: Inertial Measurement Unit (IMU) for current position and orientation\r\n- **Programming Languages**: C++ for firmware and control mechanisms, Python for the deep learning model\r\n- **Power Management**: Ensures peak wattage draw does not exceed average power generated\r\n- **Location Detection**: Automatically determines the system's geographical location for accurate sun tracking\r\n\r\n## Mechanical Design\r\n\r\n- 3D printed components for rapid prototyping\r\n- Designed to fit within a specified volume (TBD)\r\n- Incorporates waterproofing and stability features\r\n- Accommodates all necessary equipment (PCBs, camera, servos, gears, photovoltaic cell)\r\n\r\n## Electrical Design\r\n\r\n- Utilizes two PWM servos for azimuth and attitude control\r\n- Incorporates appropriate energy storage solution (battery)\r\n- Designed to operate within the power constraints of the solar panel\r\n\r\n## Tests\r\n\r\n![image](https://github.com/user-attachments/assets/fd3bf47a-cccb-419e-9e1a-c9025847882e)\r\n\r\nhttps://github.com/user-attachments/assets/189f948e-8e01-4507-97a3-aa703d4fa4bc\r\n\r\n\r\n## Setup and Installation\r\n\r\n1. Clone the repository\r\n2. Install dependencies\r\n3. Configure hardware components\r\n4. Run the main program\r\n\r\n## Current Progress\r\n\r\n### Mechanical\r\n- [ ] Select appropriate solar panel based on wattage requirements\r\n- [ ] Select appropriate energy storage solution (battery)\r\n- [ ] Create initial design sketches\r\n- [ ] Develop preliminary mechanical design\r\n- [ ] Conduct initial FEA analysis for wind effects and stability\r\n- [ ] Implement waterproofing measures\r\n\r\n### Software / Vision\r\n- [x] Implement YOLOv5 model for sun detection\r\n- [x] Develop adaptive detection interval algorithm\r\n- [x] Implement camera management for power conservation\r\n- [x] Create fallback algorithm for adverse weather conditions\r\n- [x] Integrate automatic location detection\r\n- [ ] Optimize model for microcontroller deployment\r\n- [ ] Conduct comprehensive testing and performance analysis\r\n\r\n## Future Improvements\r\n\r\n- Further optimize deep learning model for microcontroller deployment\r\n- Enhance fallback algorithm for various weather conditions\r\n- Improve software-hardware integration\r\n- Implement more sophisticated power management techniques\r\n\r\n## Contributors\r\n\r\n- **@Ampers8nd (Justin Erd.)**: Mechanical design\r\n- **@Zhoujjh3 (Justin Zhou)**: Electrical components\r\n- **@somwrks (Ash S.)**: Deep learning development\r\n\r\n## Contributing\r\n\r\nWe welcome contributions to improve any aspect of our solar panel system. Please open an issue or submit a pull request with your suggestions.\r\n\r\n## License\r\n\r\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\r\n\r\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/AI-Solar-Panel",
      "commitCount": 43,
      "latestCommitDate": "2025-01-08T00:59:35Z",
      "topics": [
        "artificial-intelligence",
        "embedded-systems"
      ]
    },
    {
      "title": "Portfolio",
      "description": "Personal Portfolio Website powered by AI",
      "readme": "This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `pages/index.js`. The page auto-updates as you edit the file.\n\n[API routes](https://nextjs.org/docs/api-routes/introduction) can be accessed on [http://localhost:3000/api/hello](http://localhost:3000/api/hello). This endpoint can be edited in `pages/api/hello.js`.\n\nThe `pages/api` directory is mapped to `/api/*`. Files in this directory are treated as [API routes](https://nextjs.org/docs/api-routes/introduction) instead of React pages.\n\nThis project uses [`next/font`](https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details.\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Portfolio",
      "commitCount": 106,
      "latestCommitDate": "2025-01-08T00:42:20Z",
      "topics": [
        "artificial-intelligence",
        "website-development"
      ]
    },
    {
      "title": "generative-ai-python",
      "description": "My contribution to Google Gemini API Library",
      "readme": "# Google AI Python SDK for the Gemini API\n\n[![PyPI version](https://badge.fury.io/py/google-generativeai.svg)](https://badge.fury.io/py/google-generativeai)\n![Python support](https://img.shields.io/pypi/pyversions/google-generativeai)\n![PyPI - Downloads](https://img.shields.io/pypi/dd/google-generativeai)\n\nThe Google AI Python SDK is the easiest way for Python developers to build with the Gemini API. The Gemini API gives you access to Gemini [models](https://ai.google.dev/models/gemini) created by [Google DeepMind](https://deepmind.google/technologies/gemini/#introduction). Gemini models are built from the ground up to be multimodal, so you can reason seamlessly across text, images, and code. \n\n## Get started with the Gemini API\n1. Go to [Google AI Studio](https://aistudio.google.com/).\n2. Login with your Google account.\n3. [Create](https://aistudio.google.com/app/apikey) an API key.\n4. Try a Python SDK [quickstart](https://github.com/google-gemini/gemini-api-cookbook/blob/main/quickstarts/Prompting.ipynb) in the [Gemini API Cookbook](https://github.com/google-gemini/gemini-api-cookbook/).\n5. For detailed instructions, try the \n[Python SDK tutorial](https://ai.google.dev/tutorials/python_quickstart) on [ai.google.dev](https://ai.google.dev).\n\n## Usage example\nSee the [Gemini API Cookbook](https://github.com/google-gemini/gemini-api-cookbook/) or [ai.google.dev](https://ai.google.dev) for complete code.\n\n1. Install from [PyPI](https://pypi.org/project/google-generativeai).\n\n`pip install -U google-generativeai`\n\n2. Import the SDK and configure your API key.\n\n```python\nimport google.generativeai as genai\nimport os\n\ngenai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n```\n\n3. Create a model and run a prompt.\n\n```python\nmodel = genai.GenerativeModel('gemini-1.5-flash')\nresponse = model.generate_content(\"The opposite of hot is\")\nprint(response.text)\n```\n\n## Documentation\n\nSee the [Gemini API Cookbook](https://github.com/google-gemini/gemini-api-cookbook/) or [ai.google.dev](https://ai.google.dev) for complete documentation.\n\n## Contributing\n\nSee [Contributing](https://github.com/google/generative-ai-python/blob/main/CONTRIBUTING.md) for more information on contributing to the Google AI Python SDK.\n\n## License\n\nThe contents of this repository are licensed under the [Apache License, version 2.0](http://www.apache.org/licenses/LICENSE-2.0).\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/generative-ai-python",
      "commitCount": 319,
      "latestCommitDate": "2024-12-26T18:20:46Z",
      "topics": [
        "library-development",
        "artificial-intelligence"
      ]
    },
    {
      "title": "OwnBoon",
      "description": "A webapp for individuals to meet self improvement content and community with many features solely dedicated to their journey powered by AI",
      "readme": "# OwnBoon\n\nOfficial Repository for OwnBoon Startup Website (Archived)\n\n## A Message to Our Developers\n\nThank you to all the developers for your amazing work on developing the OwnBoon app. Your contributions were truly invaluable, and we appreciate every single moment we had at OwnBoon. It was a fun and exciting journey.\n\n## Project Status\n\nAs of December 23, 2024, OwnBoon is no longer in business. This repository has been made public as an archive for reference purposes.\n \n\n## Reflections on Our Initial Challenges\n\n- Lack of a clear main identity or unique feature for our product\n- Slow development progress\n- Insufficient in-depth research experience\n\n## Project Hibernation\n\nThe project entered a long pause or hibernation phase before the decision to close operations.\n\n\nWe thank everyone who was part of this journey and contributed to the OwnBoon project.\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/OwnBoon",
      "commitCount": 180,
      "latestCommitDate": "2024-12-23T18:46:17Z",
      "topics": [
        "artificial-intelligence",
        "website-development"
      ]
    },
    {
      "title": "devil-guard",
      "description": "discord bot @Forkman project for asu discord server that utilizes rag and other functionalities for tracking user activities for useful data",
      "readme": "# Forkman\n\n> WARNING: Forkman is still in early development, and is not yet\nready for use in production. BE CAREFUL!!\n\nForkman is a self-hosted, open source Discord bot that is designed\nfor enterprise use in mind. As Discord becomes more of a social media platform\nthan an IRC clone, Discord bots have filled crucial needs in moderation,\nanalytics, security and more. The problem with today's Discord bots is that\nif they get too big they end up offering NFT advertisements to your users.\nThis bot aims to be simple to understand, and easy to self host.\n\nForkman is *just* a Go binary that serves a REST API for a web dashboard,\nand also opens a websocket connection to Discord to interact there. The authentication\nfor our web dashboard is hand rolled and only accepts Discord logins. The server uses\nsqlite as its just the simplest database to understand. The web dashboard is just a\nReact SPA, that interacts with that REST API. The long term goal here is that you can\nthrow Forkman on a server of your choice and forget about it.\n\n*Features*\n\n- None yet lol\n\n## Development\n\n**Roadmap:**\n\n- web dashboard\n- client-server sync for realtime updates on commands to dashboard\n- aggregate logs & open rest endpoint to collect them\n- setup case system for moderation actions\n- extend base commands\n- roll up a good ol' auth system (session based 🙅 no jwts)\n- backup system (with some sort of zip file like pocketbase)\n\n## Deps\n\n- go\n- air\n- pnpm\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/devil-guard",
      "commitCount": 101,
      "latestCommitDate": "2024-12-18T22:02:55Z",
      "topics": [
        "artificial-intelligence",
        "bot-development"
      ]
    },
    {
      "title": "pear-landing-page",
      "description": "My contribution to Landing page for PearAI, the Open Source AI-Powered Code Editor",
      "readme": "<a name=\"readme-top\"></a>\n\n<div align=\"center\">\n  <div align=\"center\">\n    <img src=\"components/ui/PearHeroLogo.svg\" alt=\"PearAI Logo\" />\n  </div>\n  <h3 align=\"center\">PearAI Landing Page</h3>\n  <p align=\"center\">\n    The Open Source AI-powered code editor\n    <br />\n    <a href=\"https://trypear.ai\"><strong>Explore the Website »</strong></a>\n    <br />\n    <br />\n    <a href=\"https://github.com/trypear/pear-landing-page/issues\">Report Bug</a>\n    ·\n    <a href=\"https://github.com/trypear/pear-landing-page/issues\">Request Feature</a>\n  </p>\n</div>\n\n---\n\n## Table of Contents\n\n- [About the Project](#about-the-project)\n  - [Built With](#built-with)\n- [Getting Started](#getting-started)\n  - [Prerequisites](#prerequisites)\n  - [Installation](#installation)\n  - [Environment Variables](#environment-variables-description)\n- [Usage](#usage)\n- [Contributing](#contributing)\n- [Contact](#contact)\n- [Acknowledgements](#acknowledgements)\n\n## About The Project\n\nThis is the landing page for [PearAI:](https://trypear.ai) the Open Source AI-powered code editor. This product is managed by [Nang](https://youtube.com/nang88) and [Pan](https://youtube.com/FryingPan).\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n### Built With\n\n- [![nextjs][nextjs]][nextjs-url]\n- [![vercel][vercel]][vercel-url]\n- [![tailwindcss][tailwindcss]][tailwindcss-url]\n- [![typescript][typescript]][typescripturl]\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n## Getting Started\n\nTo get a local copy up and running follow these simple steps.\n\n### Prerequisites\n\n- Yarn\n  ```sh\n  npm install --global yarn\n  ```\n\n### Installation\n\n1.  Clone the repo\n    ```sh\n    git clone https://github.com/trypear/pear-landing-page.git\n    ```\n2.  Install NPM packages\n    ```sh\n    yarn install\n    ```\n    <p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n### Project Configuration\n\nThis project requires environment variables to be set up in a `.env.local` file for proper configuration and operation. Below are the required environment variables and instructions on how to set them up.\n\n### Required Environment Variables\n\n1. `NEXT_PUBLIC_SUPABASE_URL`\n2. `NEXT_PUBLIC_SUPABASE_ANON_KEY`\n\n### Environment Variables Description\n\n- **NEXT_PUBLIC_SUPABASE_URL**: This is the URL of your Supabase project.\n\n  Example: `NEXT_PUBLIC_SUPABASE_URL=https://xyzcompany.supabase.co`\n\n- **NEXT_PUBLIC_SUPABASE_ANON_KEY**: This is the anonymous public key for your Supabase project. This key allows your frontend application to interact with the Supabase backend.\n\n  Example: `NEXT_PUBLIC_SUPABASE_ANON_KEY=your-anon-key`\n\n- **NEXT_PUBLIC_VERCEL_URL**: This is the URL to which users will be redirected after certain actions, such as authentication. During local development, this is typically `http://localhost:3000`. For Vercel preview/dev deployments, it will be whatever URL Vercel generates. For production, we should use `NEXT_PUBLIC_SITE_URL` instead. Both `NEXT_PUBLIC_SITE_URL` and `NEXT_PUBLIC_VERCEL_URL` are auto-generated by Vercel, so no need to worry about it.\n\n  Example: `NEXT_PUBLIC_REDIRECT_URL=http://localhost:3000`\n\n## Usage\n\nTo run the project locally:\n\n1. Start the development server\n   ```sh\n   yarn dev\n   ```\n2. Visit `http://localhost:3000` in your browser.\n\n### Recommended Extensions\n\n- Prettier\n  - Open your command palette, choose your default formatter to be Prettier, and enable format on save.\n- ESLint\n  - When you push a commit, we have a pre-commit hook that automatically runs prettier, eslint, and builds your project to make sure everything is ok.\n- JavaScript and TypeScript Nightly\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n## Contributing\n\nContributions are what make the open source community such an amazing place to be, learn, inspire, and create. Any contributions you make are **greatly appreciated**.\n\nIf you have a suggestion that would make this better, please fork the repo and create a pull request.\n\n1. Fork the repo\n   ```sh\n   git clone https://github.com/trypear/pear-landing-page.git\n   ```\n2. Clone the repo\n   ```sh\n   git clone https://github.com/<USERNAME>/pear-landing-page.git\n   ```\n3. Navigate to the project directory\n   ```sh\n   cd pear-landing-page\n   ```\n4. Create a new branch\n   ```sh\n   git checkout -b my-new-branch\n   ```\n5. Install dependencies\n   ```sh\n   yarn install\n   ```\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n## Contact\n\n- Nang - [@youtube/nang88](https://youtube.com/nang88)\n- Pan - [@youtube/FryingPan](https://youtube.com/FryingPan)\n- [Discord](https://discord.com/invite/7QMraJUsQt)\n- Email - pear@trypear.ai\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n[typescript]: https://img.shields.io/badge/typescript-%23007ACC.svg?style=for-the-badge&logo=typescript&logoColor=white\n[typescripturl]: https://www.typescriptlang.org/\n[vercel]: https://img.shields.io/badge/Vercel-%23000000.svg?style=for-the-badge&logo=vercel&logoColor=white\n[vercel-url]: https://vercel.com/\n[nextjs]: https://img.shields.io/badge/Next.js-%23000000.svg?style=for-the-badge&logo=next.js&logoColor=white\n[nextjs-url]: https://nextjs.org/\n[tailwindcss]: https://img.shields.io/badge/Tailwind_CSS-%231a202c.svg?style=for-the-badge&logo=tailwind-css&logoColor=white\n[tailwindcss-url]: https://tailwindcss.com/\n[discord]: https://img.shields.io/badge/typescript-%23007ACC.svg?style=for-the-badge&logo=discord&logoColor=white\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/pear-landing-page",
      "commitCount": 1228,
      "latestCommitDate": "2024-12-06T09:28:50Z",
      "topics": [
        "artificial-intelligence",
        "website-development"
      ]
    },
    {
      "title": "Self-Driving-Car-Simulation",
      "description": "neural network car simulation through traffic",
      "readme": "# Self-Driving Car Simulation\n\nA fascinating deep learning visualization project that demonstrates the beauty of neural networks learning to navigate through traffic in real-time. This project brings together the elegance of autonomous driving and the complexity of neural evolution.\n\n## Overview\n\nThis simulator creates an environment where AI-controlled cars learn to navigate through traffic using neural networks. Each car is equipped with sensors and a brain that evolves over time, showcasing the remarkable ability of deep learning systems to adapt and improve through experience.\n\n### Neural Network Visualization\n\n- **Real-time Brain Activity**: Watch neural networks think and make decisions as cars navigate\n- **Interactive Network Display**: Visualizes neuron activations, weights, and biases in real-time\n- **Sensor System**: Shows how cars perceive their environment through multiple sensors\n\n### Learning Environment\n\n- **Dynamic Traffic**: Add or remove traffic cars to create varying levels of complexity\n- **Speed Control**: Adjust traffic speed to create different learning scenarios\n- **Mutation System**: Cars learn through genetic evolution, with each generation improving upon the last\n\n### Training Controls\n\n- **Save Best Performer**: Preserve the neural network of the most successful car\n- **Parallel Learning**: Multiple cars learn simultaneously, accelerating the evolution process\n- **Progressive Difficulty**: Traffic patterns that challenge the AI to develop robust driving strategies\n\n![{7712F6A4-D2A4-4FDC-95A8-17B469AA8670}](https://github.com/user-attachments/assets/77efdf3f-b62b-44e6-a183-1b7f95afa428)\n\n\n## Core Components\n\n**Canvas Setup**\n\n- Uses two HTML canvases: one for the driving simulation and another for neural network visualization[1]\n- The world is rendered using 2D canvas context with a road and multiple car objects\n\n**Car Physics**\n\n- Each car is represented by a polygon with four points forming a rectangle[1]\n- Cars have properties like position (x, y), dimensions (width, height), speed, acceleration, and angle[1]\n- Collision detection is implemented using polygon intersection checks\n\n## Neural Network Architecture\n\n**Sensor System**\n\n### Network Structure\n\n```jsx\nclass NeuralNetwork {\n  constructor(neuronCounts) {\n    this.levels = [];\n    for (let i = 0; i < neuronCounts.length - 1; i++) {\n      this.levels.push(new Level(neuronCounts[i], neuronCounts[i + 1]));\n    }\n  }\n}\n```\n\n**Network Structure**\n\n- Input layer: 5 neurons (sensor readings)\n- Hidden layer: 6 neurons\n- Output layer: 4 neurons (controls)\n- Each level contains weights and biases\n- Fully connected between layers\n\n### Feedforward Process\n\n```jsx\nstatic feedForward(givenInputs, network) {\n    let outputs = Level.feedForward(givenInputs, network.levels[0]);\n    for (let i = 1; i < network.levels.length; i++) {\n        outputs = Level.feedForward(outputs, network.levels[i]);\n    }\n    return outputs;\n}\n\nstatic feedForward(givenInputs, level) {\n    for (let i = 0; i < level.inputs.length; i++) {\n        level.inputs[i] = givenInputs[i];\n    }\n\n    for (let i = 0; i < level.outputs.length; i++) {\n        let sum = 0;\n        for (let j = 0; j < level.inputs.length; j++) {\n            sum += level.inputs[j] * level.weights[j][i];\n        }\n\n        if (sum > level.biases[i]) {\n            level.outputs[i] = 1;\n        } else {\n            level.outputs[i] = 0;\n        }\n    }\n\n    return level.outputs;\n}\n\n```\n\nNeural processing:\n\n- Inputs are sensor readings (distances to obstacles)\n- Each neuron computes weighted sum of inputs\n- Binary activation based on bias threshold\n- Output determines car controls:\n  - Forward movement\n  - Left turn\n  - Right turn\n  - Reverse movement\n- Each car has 5 raycast sensors that detect obstacles and road boundaries[1]\n- Sensors cast rays at different angles, measuring distances to obstacles\n- Each ray returns an offset value (distance to detected object) which serves as input to the neural network\n\n## Learning Mechanism\n\n**Genetic Algorithm**\n\n- Multiple cars (N=100) are generated simultaneously with randomized neural networks[1]\n- Each car's \"fitness\" is determined by how far it travels without crashing\n- The best-performing car's neural network (weights and biases) is saved as the \"best brain\"\n\n**Mutation Process**\n\n```jsx\nstatic mutate(network, amount = 1) {\n    network.levels.forEach((level) => {\n        // Mutate biases\n        for (let i = 0; i < level.biases.length; i++) {\n            level.biases[i] = lerp(\n                level.biases[i],\n                Math.random() * 2 - 1,\n                amount\n            );\n        }\n        // Mutate weights\n        for (let i = 0; i < level.weights.length; i++) {\n            for (let j = 0; j < level.weights[i].length; j++) {\n                level.weights[i][j] = lerp(\n                    level.weights[i][j],\n                    Math.random() * 2 - 1,\n                    amount\n                );\n            }\n        }\n    });\n}\n\n```\n\n- Mutation occurs by slightly modifying the weights and biases of the neural network\n- Uses linear interpolation (lerp) to create variations of the successful network\n- Mutation amount (0.1) determines how much the networks can vary from the original[1]\n\n## Car Physics and Control\n\n**Car Properties**\n\n```jsx\n#createPolygon() {\n    const points = [];\n    const rad = Math.hypot(this.width, this.height) / 2;\n    const alpha = Math.atan2(this.width, this.height);\n\n    // Create 4 corners of the car\n    points.push({\n        x: this.x - Math.sin(this.angle - alpha) * rad,\n        y: this.y - Math.cos(this.angle - alpha) * rad\n    });\n    // ... (similar calculations for other 3 points)\n\n    return points;\n}\n\n```\n\n- Each car is represented by a polygon with four points\n- Properties include:\n  - Position (x, y)\n  - Dimensions (width, height)\n  - Speed and acceleration\n  - Friction coefficient\n  - Damage state[1]\n\n```jsx\n#move() {\n    // Forward/Backward movement\n    if (this.controls.forward) this.speed += this.acceleration;\n    if (this.controls.reverse) this.speed -= this.acceleration;\n\n    // Speed limits\n    if (this.speed > this.maxSpeed) this.speed = this.maxSpeed;\n    if (this.speed < -this.maxSpeed/2) this.speed = -this.maxSpeed/2;\n\n    // Friction\n    if (this.speed > 0) this.speed -= this.friction;\n    if (this.speed < 0) this.speed += this.friction;\n    if (Math.abs(this.speed) < this.friction) this.speed = 0;\n\n    // Turning\n    if (this.speed != 0) {\n        const flip = this.speed > 0 ? 1 : -1;\n        if (this.controls.left) this.angle += 0.03 * flip;\n        if (this.controls.right) this.angle -= 0.03 * flip;\n    }\n\n    // Update position\n    this.x -= Math.sin(this.angle) * this.speed;\n    this.y -= Math.cos(this.angle) * this.speed;\n}\n\n```\n\n**Movement Control**\n\n- The neural network outputs control signals:\n  - Forward acceleration\n  - Left/right steering\n  - Reverse movement\n- These controls are applied through physics calculations considering:\n  - Acceleration and friction\n  - Angular rotation for steering\n  - Collision detection with road boundaries and other cars[1]\n\nThe movement system includes:\n\n- Acceleration and deceleration based on controls\n- Maximum speed limits (different for forward/reverse)\n- Friction to slow the car naturally\n- Turning mechanics that depend on speed direction\n- Position updates based on angle and speed\n\n### Ray Casting\n\n```jsx\nclass Sensor {\n  constructor(car) {\n    this.car = car;\n    this.rayCount = 5;\n    this.rayLength = 150;\n    this.raySpread = Math.PI / 2; // 90 degrees\n  }\n\n  #castRays() {\n    this.rays = [];\n    for (let i = 0; i < this.rayCount; i++) {\n      const rayAngle =\n        lerp(\n          this.raySpread / 2,\n          -this.raySpread / 2,\n          this.rayCount == 1 ? 0.5 : i / (this.rayCount - 1)\n        ) + this.car.angle;\n\n      const start = { x: this.car.x, y: this.car.y };\n      const end = {\n        x: this.car.x - Math.sin(rayAngle) * this.rayLength,\n        y: this.car.y - Math.cos(rayAngle) * this.rayLength,\n      };\n      this.rays.push([start, end]);\n    }\n  }\n}\n```\n\nSensor implementation:\n\n- Creates 5 rays in a fan pattern\n- Rays spread across 90 degrees\n- Each ray extends 150 units from car\n- Ray angles are interpolated evenly across spread\n- Updates with car position and rotation\n\n### Intersection Detection\n\n```jsx\n#getReading(ray, roadBorders, traffic) {\n    let touches = [];\n\n    // Check road borders\n    for (let i = 0; i < roadBorders.length; i++) {\n        const touch = getIntersection(\n            ray[0], ray[1],\n            roadBorders[i][0], roadBorders[i][1]\n        );\n        if (touch) touches.push(touch);\n    }\n\n    // Check traffic\n    for (let i = 0; i < traffic.length; i++) {\n        const poly = traffic[i].polygon;\n        for (let j = 0; j < poly.length; j++) {\n            const touch = getIntersection(\n                ray[0], ray[1],\n                poly[j], poly[(j+1)%poly.length]\n            );\n            if (touch) touches.push(touch);\n        }\n    }\n\n    // Return closest intersection\n    if (touches.length == 0) return null;\n    const offsets = touches.map(e => e.offset);\n    const minOffset = Math.min(...offsets);\n    return touches.find(e => e.offset == minOffset);\n}\n\n```\n\nIntersection detection:\n\n- Checks each ray against road borders and traffic\n- Uses line-segment intersection algorithm\n- Stores distance (offset) to each intersection\n- Returns closest intersection point\n- Null if no intersections found\n\n## Traffic System\n\n### Traffic Generation\n\n```jsx\nfunction generateCars(N, road) {\n  const cars = [];\n  for (let i = 1; i <= N; i++) {\n    cars.push(new Car(road.getLaneCenter(1), 100, 30, 50, \"AI\"));\n  }\n  return cars;\n}\n\nconst addTrafficCar = () => {\n  const randomLane = Math.floor(Math.random() * 3);\n  const bestCarY = Math.min(...carsRef.current.map((car) => car.y));\n  const minY = bestCarY - window.innerHeight * 0.3;\n\n  const newCar = new Car(\n    road.getLaneCenter(randomLane),\n    minY,\n    30,\n    50,\n    \"DUMMY\",\n    2,\n    getRandomColor()\n  );\n\n  setTraffic((prevTraffic) => [...prevTraffic, newCar]);\n};\n```\n\n- Generates dummy cars in random lanes\n- Places new cars ahead of best AI car\n- Removes cars too far behind\n- Adjustable traffic speed\n- Random colors for visual distinction\n- Traffic cars are generated with:\n  - Random lane positions\n  - Constant forward movement\n  - Different colors for visibility\n  - Simplified AI (DUMMY control type)\n\n**Dynamic Traffic Management**\n\n- New traffic cars can be added during simulation\n- Traffic speed can be adjusted\n- Cars too far behind are automatically removed\n- Traffic positions are relative to the best-performing car[1]\n\n## Visualization\n\nThis simulation demonstrates the integration of multiple complex systems working together to create an evolving, learning self-driving car system. The combination of physics simulation, neural network processing, and evolutionary learning allows the cars to develop increasingly sophisticated driving behaviors over time.\n\n**Main Canvas**\n\n- Shows the road, cars, and sensors\n- Camera follows the best-performing car\n- Displays sensor rays and their intersections\n- Shows collision detection in real-time\n\n**Network Visualization**\n\n```jsx\nstatic drawNetwork(ctx, network) {\n    const margin = 50;\n    const left = margin;\n    const top = margin;\n    const width = ctx.canvas.width - margin * 2;\n    const height = ctx.canvas.height - margin * 2;\n    const levelHeight = height / network.levels.length;\n\n    // Draw levels\n    for (let i = network.levels.length - 1; i >= 0; i--) {\n        const levelTop = top + lerp(\n            height - levelHeight,\n            0,\n            network.levels.length == 1\n                ? 0.5\n                : i / (network.levels.length - 1)\n        );\n\n        ctx.setLineDash([7, 3]);\n        Visualizer.drawLevel(\n            ctx, network.levels[i],\n            left, levelTop,\n            width, levelHeight,\n            i == network.levels.length - 1\n                ? [\"🠉\", \"🠈\", \"🠊\", \"🠋\"]\n                : []\n        );\n    }\n}\n\n```\n\nVisualization features:\n\n- Real-time neural network display\n- Color-coded connections\n- Node activation visualization\n- Control output indicators\n- Sensor ray rendering\n- Network architecture display\n- Displays neural network structure\n- Shows active connections and neuron states\n- Visualizes weights through line thickness and color\n- Indicates bias values through node borders\n\n## Technical Implementation\n\nThe project leverages several key concepts in deep learning:\n\n- **Feedforward Neural Networks**: Multi-layer perceptron architecture for decision making\n- **Genetic Algorithms**: Evolution-based learning through mutation and selection\n- **Sensor Fusion**: Multiple raycast sensors providing environmental awareness\n- **Real-time Visualization**: Canvas-based rendering of both the environment and neural network state\n\n## Why This Matters\n\nThis simulator serves as a window into the learning process of neural networks, making typically abstract concepts tangible and observable. It demonstrates how:\n\n- Neural networks develop decision-making capabilities through experience\n- Simple rules can lead to complex, intelligent behavior\n- Machine learning systems adapt to changing environments\n- Collective learning can emerge from individual experiences\n\n## Future Enhancements\n\nThe project is continuously evolving, with planned features including:\n\n- Advanced learning algorithms\n- More complex traffic scenarios\n- Additional sensor types\n- Performance metrics and analytics\n- Training data export capabilities\n\n## Getting Started\n\n1. Clone the repository\n2. Install dependencies with `npm install`\n3. Run the development server with `npm run dev`\n4. Open your browser to experience the learning process\n\n## Contributing\n\nYour contributions to improve this learning environment are welcome! Whether it's adding features, improving the learning algorithm, or enhancing the visualization, every contribution helps create a better understanding of neural networks.\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Self-Driving-Car-Simulation",
      "commitCount": 20,
      "latestCommitDate": "2024-10-31T06:38:49Z",
      "topics": [
        "artificial-intelligence",
        "website-development"
      ]
    },
    {
      "title": "RL-Maze",
      "description": " Reinforcement Learning Agent for simulating maze ",
      "readme": "![Screenshot 2024-06-17 225828](https://github.com/somwrks/RL-Maze/assets/85481905/8576bc64-63be-40b7-bed3-332e47ebb367)\n\n\n![Screenshot 2024-06-17 225617](https://github.com/somwrks/RL-Maze/assets/85481905/8fecc84d-d91a-465f-baea-2e50f72f4370)\n\n![Screenshot 2024-06-17 225657](https://github.com/somwrks/RL-Maze/assets/85481905/41f11b44-f18b-450f-9e16-caf492a10366)\n\n\n![gif](https://github.com/somwrks/RL-Maze/assets/85481905/411b98b3-c613-4ce2-9e2f-170e44c133f0)\n\n\n# RL-Maze Simulation\n\nThis project is a Reinforcement Learning (RL) simulation project built with a combination of Next.js, Flask, Tailwind CSS, and OpenCV. The purpose is to provide an educational platform for machine learning enthusiasts, allowing them to learn about RL by building their own mazes and training AI agents to solve them.  \n\nThe application allows users to create custom mazes with defined walls, reward points, and step penalties. Users can then select from a variety of RL algorithms and configure their parameters. The website will then visualize the agent learning process as it navigates the maze, aiming to reach the designated endpoint. The goal is to provide an intuitive and interactive way for users to understand the concepts of RL and see how agents learn to solve complex problems.\n\n\n## Overview\n\nThis project is an educational reinforcement learning simulation that allows users to experiment with building mazes and training agents to navigate them. Users can define the maze layout, set reward and penalty points, and observe how the agent learns to find the optimal path to the goal.  The simulation visualizes the agent's progress and provides insights into the reinforcement learning process.  The project leverages NextJS for the frontend, Flask for the backend, TailwindCSS for styling, and OpenCV for image processing. Users can build their own mazes, set parameters for their agent, and watch it learn and solve the maze. The simulation visualizes the agent's progress and provides insights into the reinforcement learning process. \n\n\n## Dependencies\n\nThis project is an educational tool that allows users to build their own mazes and train reinforcement learning agents to solve them. Users can define the maze's layout, set up reward and penalty points, and observe the agent's learning process as it navigates the maze. The project uses NextJS for the frontend, Flask for the backend, and OpenCV for visual processing. This combination creates an interactive and informative experience for users interested in reinforcement learning concepts. The project is under development and welcomes contributions. \n\n\n## Usage\n\n1. **Ensure all dependencies are installed.** You can install all dependencies by running `npm install` in the project's root directory.\n2. **Build the application** by running `npm run build` in the project's root directory.\n3. **Launch the application** by running `npm run start` in the project's root directory. \n4. **Run `python index.py`** in the project's root directory to start the Flask server.\n5. **Access the application in your web browser** at `http://localhost:5000/`. \n6. **Build your own maze** using the provided interface, and set parameters for your reinforcement learning agent, including wall positions, step costs, and reward points.\n7. **Watch as your agent learns to navigate the maze** and see the progress of its exploration and solution. \n\n\n## Code Structure\n\nThe key components of the code include:\n\n- **UI:** The application's user interface is built using `NextJS`, which leverages the power of `Next.js` for building web applications and integrates it with `ElectronJS` to create a native desktop application. The UI is styled using `TailwindCSS` for a modern and customizable look.\n- **Maze Logic:**  The `index.py` and `main.py` files contain the core logic for generating, displaying, and solving the maze. The `TypingText.tsx` and `middleware.ts` files handle user interaction and data flow between the frontend and backend. \n- **API Integration:** The project uses `OpenCV` for image processing and maze visualization, and the code interacts with the user's provided parameters for maze creation and agent behavior.\n- **User Interaction:** Users can input their own maze dimensions, wall positions, reward points, and step penalty values. This allows for customization and exploration of different maze environments.\n- **Agent Learning:**  The application utilizes reinforcement learning principles to enable the agent to learn the optimal path through the maze. The progress of the agent is visualized on the website as it learns and navigates the maze. \n\n\n\n## Folder Structure\n\n- `.gitignore`: Contains version control information for the project.\n- `index.py`: Main Python file for the simulation logic.\n- `main.py`:  Entry point for the Flask server.\n- `TypingText.tsx`:  Component for displaying and managing text typing animation.\n- `middleware.ts`:  Middleware functions for handling requests and responses.\n- `next.config.js`:  Configuration file for Next.js, defining build settings and optimizations.\n- `package-lock.json`: Contains information about the project's dependencies and their versions.\n- `package.json`: The project's manifest file, defining dependencies, scripts, and other configuration.\n- `pnpm-lock.yaml`:  Pnpm lock file managing package dependencies.\n- `postcss.config.js`: Configuration file for PostCSS, defining styling rules and plugins.\n- `requirements.txt`: Lists Python dependencies for the project.\n- `tailwind.config.js`: Configuration file for TailwindCSS, defining the styling framework.\n- `tsconfig.json`: TypeScript configuration file, defining compiler settings.\n- `renderer`: Contains the frontend code built with Next.js, TailwindCSS, and OpenCV. \n- `components`: Contains reusable UI components for the web application.\n- `pages`: Contains the application's pages including the homepage for user interaction with the maze simulation.\n- `api`: Contains API routes for interacting with the simulation logic and managing user input.\n- `public`: Contains static assets for the application like images.\n- `styles`: Contains global styles for the application.\n- `globals.css`: Global CSS styles.\n\n\n## License\n\nThis project is licensed under the [MIT License](https://opensource.org/licenses/MIT). Feel free to modify and distribute the code as per the terms of the license. \n\n\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/RL-Maze",
      "commitCount": 38,
      "latestCommitDate": "2024-10-31T03:09:21Z",
      "topics": [
        "artificial-intelligence",
        "website-development"
      ]
    },
    {
      "title": "Fashion-Images-Generator",
      "description": "Deep Learning Project for analyzing various images and objects to generate new images or objects with the use of Generative Adversarial Neural Network",
      "readme": "# Fashion Images Generator (Generative Adversarial Neural Network)\n\nThis project focuses on implementing a Generative Adversarial Neural Network (GAN) for deep learning applications, specifically for analyzing various images and objects to generate new images or objects. The project is based on TensorFlow and TensorFlow Datasets, using a fashion MNIST dataset for training.\n\n![image](https://github.com/user-attachments/assets/091df104-92be-4db0-8666-567eec4cc102)\n\n![image](https://github.com/user-attachments/assets/4a4f1e1f-2c6b-42c6-b3dc-21021cbba510)\n\n## Overview\n\nThe project consists of the following components:\n\n1. `main.ipynb`: This Jupyter Notebook file contains the main codebase for building and training the GAN model.\n\n## Dependencies\n\nThe project requires the following dependencies:\n\n- TensorFlow (version >= 2.x)\n- TensorFlow Datasets\n- Matplotlib\n- NumPy\n- Pandas\n\n## Usage\n\n1. Ensure all dependencies are installed.\n2. Open and run the `main.ipynb` notebook in a Jupyter environment.\n3. Follow the instructions and code comments within the notebook to train and evaluate the GAN model.\n\n## Code Structure\n\nThe key components of the code include:\n\n- Data preprocessing and loading using TensorFlow Datasets.\n- Building the generator and discriminator models.\n- Compiling the GAN model and defining the training process.\n- Monitoring the training progress using custom callbacks.\n- Saving and loading model weights for further use.\n\n## Folder Structure\n\n- `archive`: Contains saved model weights.\n- `images`: Generated images during training.\n\n## How to Contribute\n\nContributions to this project are welcome! If you have any suggestions, improvements, or bug fixes, please feel free to create issues or pull requests.\n\n## License\n\nThis project is licensed under the MIT License. Feel free to modify and distribute the code as per the terms of the license.\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Fashion-Images-Generator",
      "commitCount": 13,
      "latestCommitDate": "2024-10-08T23:13:00Z",
      "topics": [
        "artificial-intelligence"
      ]
    },
    {
      "title": "Recommender-System",
      "description": "Machine Learning Project for recommending best books",
      "readme": "# AI Book Recommendation System\n\n## Overview\n\nThis project is a **Book Recommendation System** powered by machine learning. It takes user preferences based on previously read books and generates recommendations. The system utilizes deep learning models to predict and recommend similar books that the user may enjoy.\n\n## Features\n\n- Accepts a list of books previously read by a user\n- Recommends a set of books based on the user's reading history\n- Leverages deep learning models for natural language understanding and book similarity prediction\n\n## Technologies Used\n\n- **Flask**: Backend framework for the API server\n- **Keras** and **TensorFlow**: Deep learning libraries used to build and train the recommendation model\n- **Python**: General-purpose programming language for the backend and machine learning pipeline\n- **Anaconda**: Python distribution for managing packages and environments\n\n## Getting Started\n\n### Prerequisites\n\nTo run this project, you will need to have the following installed:\n\n- Python 3.8+\n- Flask\n- TensorFlow\n- Keras\n- Anaconda (for environment management)\n\n### Installation\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/your-username/book-recommendation-system.git\n   cd book-recommendation-system\n   ```\n\n2. Create a virtual environment and activate it:\n   ```bash\n   conda create -n book-recommender python=3.8\n   conda activate book-recommender\n   ```\n\n3. Install the required dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. Run the Flask server:\n   ```bash\n   python app.py\n   ```\n\n### Usage\n\nOnce the Flask server is running, you can send a POST request to the `/recommend` endpoint with the following structure:\n\n```json\n{\n  \"user_id\": \"user12345\",\n  \"books\": [\n    \"The Catcher in the Rye\",\n    \"To Kill a Mockingbird\",\n    \"1984\"\n  ]\n}\n```\n\nThe system will return a list of recommended books based on the input.\n\n## Machine Learning/Deep Learning Models\n\n### Embedding Layers\n\nThe project uses an embedding layer to convert book titles into dense vectors that capture semantic similarities between different books. The embedding layer is trained to learn meaningful representations of books based on user interactions and preferences.\n\n### Recurrent Neural Networks (RNN)\n\nWe use a Recurrent Neural Network (RNN) model with LSTM (Long Short-Term Memory) units to analyze the sequence of books read by users and understand their preferences. The RNN helps capture the patterns in user behavior, identifying trends like genre preferences or favorite authors.\n\n### Collaborative Filtering\n\nCollaborative filtering is used to identify similar users and recommend books that users with similar tastes have enjoyed. This approach helps the model generalize recommendations beyond the specific books the user has read.\n\n### Matrix Factorization\n\nIn addition to collaborative filtering, matrix factorization techniques like Singular Value Decomposition (SVD) are used to uncover hidden factors in the user-book interaction matrix. These factors represent latent features that describe both the books and users, helping in more accurate recommendations.\n\n### Model Training\n\nThe deep learning model is trained using historical data of user interactions with books. The training process involves minimizing the prediction error between the recommended books and the books users have actually enjoyed. The loss function used for training is typically mean squared error (MSE) for regression-based predictions.\n\n## Example Output\n\n```json\n{\n  \"user_id\": \"user12345\",\n  \"recommendations\": [\n    \"The Great Gatsby\",\n    \"Pride and Prejudice\",\n    \"Brave New World\"\n  ]\n}\n```\n\n## Known Issues\n\n- **Index Out of Range Error**: Occurs when the input indices exceed the size of the embedding layer. This issue arises due to mismatch in tokenization or vocabulary size. Ensure that the tokenization process matches the model's expected input format.\n\n## Contributing\n\nFeel free to fork this repository and submit pull requests for any bug fixes or improvements!\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Recommender-System",
      "commitCount": 12,
      "latestCommitDate": "2024-10-07T23:22:00Z",
      "topics": [
        "artificial-intelligence"
      ]
    },
    {
      "title": "Carbon-Emission-Detector",
      "description": "mobile app to detect carbon emissions nerd stats of cars using deep learning",
      "readme": "# Carbon Emission Detector App\n\nThis project is a Kivy-based mobile app that uses a pre-trained Faster R-CNN model for car detection and estimates carbon emissions based on the detected vehicles. It leverages OpenCV for image processing and provides live updates of the detected cars and their corresponding estimated carbon footprint.\n\n### Here's what the code is trying to do-\n\n1. Use a pre-trained Faster R-CNN model from torchvision for car detection.\n2. Process camera input in real-time using a Kivy app.\n3. Detect cars in the camera feed.\n4. Estimate the size/class of the detected car.\n5. Predict carbon emissions based on the estimated car size using a Canadian CO2 emissions dataset.\n6. Display the results in real-time, including the detected car's class and estimated CO2 emissions.\n\n![image](https://github.com/user-attachments/assets/19bdaebc-4840-499f-9551-9045b36d0ee2)\n\n![image](https://github.com/user-attachments/assets/72c4d01a-4774-41ed-80e4-3351e00bd928)\n\n## Run the Project\n\nFollow these steps to set up the development environment and run the project:\n\n### Prerequisites\nEnsure you have Python installed on your machine. Then, follow these steps:\n\n1. Install required dependencies:\n   ```\n   pip install kivy opencv-python numpy torch torchvision pillow pandas\n\n   ```\n\n2. Set up a virtual environment:\n   ```\n   python -m venv venv\n   ```\n\n3. Activate the virtual environment:\n   - On Linux/MacOS:\n     ```\n     source venv/bin/activate\n     ```\n   - On Windows:\n     ```\n     venv\\Scripts\\activate\n     ```\n\n4. Download the YOLO weights, configuration file, and class names:\n   ```\n   wget https://pjreddie.com/media/files/yolov3.weights\n   wget https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg\n   ```\n\n\n5. Run the app:\n   ```\n   python main.py\n   ```\n\n## Project Structure\n- `main.py`: The main application file containing the Kivy app and car detection logic.\n- `data/CO2_Emissions_Canada.csv`: Dataset containing CO2 emissions data for various vehicle classes.\n- `imagenet_classes.json`: JSON file containing ImageNet class names.\n\n## Usage\n\nRun the application:\n\n  The app will open a camera feed and start detecting cars. For each detected car, it will display:\n- The detected car's make/model (based on ImageNet classes)\n- Estimated vehicle class (COMPACT, MID-SIZE, or SUV - SMALL)\n- Estimated CO2 emissions in g/km\n\n## Future Improvements\n\n- Implement a more sophisticated car make and model recognition system.\n- Improve the accuracy of CO2 emissions estimation by considering more factors.\n- Optimize the app for better performance on mobile devices.\n- Expand the dataset to include a wider range of vehicles and more accurate emissions data.\n\n### How to Contribute\n\n1. **Fork the repository**:  \n   Start by forking this repository to your own GitHub account.\n\n2. **Clone the forked repository**:  \n   Clone your forked repository to your local machine:\n   ```\n   git clone https://github.com/somwrks/Carbon-Emission-Detector.git\n   ```\n\n3. **Create a new branch**:  \n   Make sure to create a new branch for your changes:\n   ```\n   git checkout -b main\n   ```\n\n4. **Make your changes**:  \n   Implement your changes in this branch. Ensure your code follows best practices and is well-documented.\n\n5. **Run tests**:  \n   Ensure that your changes don't break any existing functionality. Test the app locally before submitting a PR.\n\n6. **Commit your changes**:  \n   Use clear and concise commit messages:\n   ```\n   git commit -m \"Added feature X\" \n   ```\n\n7. **Push your changes**:  \n   Push your changes to your fork:\n   ```\n   git push origin main\n   ```\n\n8. **Submit a Pull Request (PR)**:  \n   Open a pull request to the main repository:\n   - Make sure to describe your changes clearly in the PR description.\n   - Reference any issues that are being addressed by the PR (if applicable).\n\n### Pull Request Guidelines\n\n- Ensure your PR is up-to-date with the `main` branch before submitting.\n- Add appropriate comments and documentation to your code.\n- Write meaningful commit messages.\n- Be respectful of code reviews and make requested changes promptly.\n- Please do not make any direct changes to the `main` branch.\n- Small, focused PRs are preferred over large, all-encompassing ones.\n\n### Reporting Issues\n\nIf you encounter any bugs, problems, or have feature requests, feel free to open an issue on GitHub. Please provide as much context as possible, including steps to reproduce the problem.\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Carbon-Emission-Detector",
      "commitCount": 28,
      "latestCommitDate": "2024-10-07T04:58:31Z",
      "topics": [
        "artificial-intelligence",
        "app-development"
      ]
    },
    {
      "title": "Ops",
      "description": "MLOps & DevOps Course",
      "readme": "### This is where i do mlops and devops, got any problem with that?\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Ops",
      "commitCount": 6,
      "latestCommitDate": "2024-10-06T22:08:58Z",
      "topics": [
        "notes"
      ]
    },
    {
      "title": "DL-Notes",
      "description": "All my DL course notes",
      "readme": "# All my Deep learning course notes\n\n![image](https://github.com/somwrks/Deep-Learning-Course/assets/85481905/7c4500cd-b6ed-4d91-8621-62130e657777)\n![image](https://github.com/somwrks/Deep-Learning-Course/assets/85481905/fabf17bb-502b-464a-89f4-abde4c18505c)\n![image](https://github.com/somwrks/Deep-Learning-Course/assets/85481905/5a10c825-f08b-44b8-bbd5-2f5359d7e2d3)\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/DL-Notes",
      "commitCount": 11,
      "latestCommitDate": "2024-10-01T20:46:34Z",
      "topics": [
        "notes"
      ]
    },
    {
      "title": "codeboiler",
      "description": "sun hacks' 24, ai claude to deploy boiler plated project for react with just NLP to github repo directory, supports cmd",
      "readme": "This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `pages/index.tsx`. The page auto-updates as you edit the file.\n\n[API routes](https://nextjs.org/docs/api-routes/introduction) can be accessed on [http://localhost:3000/api/hello](http://localhost:3000/api/hello). This endpoint can be edited in `pages/api/hello.ts`.\n\nThe `pages/api` directory is mapped to `/api/*`. Files in this directory are treated as [API routes](https://nextjs.org/docs/api-routes/introduction) instead of React pages.\n\nThis project uses [`next/font`](https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details.\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/codeboiler",
      "commitCount": 45,
      "latestCommitDate": "2024-09-29T10:05:51Z",
      "topics": [
        "cli-application",
        "artificial-intelligence",
        "hackathon-project"
      ]
    },
    {
      "title": "ML-Notes",
      "description": "All my ML course notes",
      "readme": "# Quick Revision\n\n### [Cheat Sheet](https://github.com/somwrks/ML-Notes/tree/master/Quick%20Revision/Cheatsheet) \n### [Lecture Notes](https://github.com/somwrks/ML-Notes/tree/master/Quick%20Revision/Lecture) \n### [Code Implementations](https://github.com/somwrks/ML-Notes/tree/master/Quick%20Revision/Code) \n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/ML-Notes",
      "commitCount": 39,
      "latestCommitDate": "2024-09-24T04:27:01Z",
      "topics": [
        "notes"
      ]
    },
    {
      "title": "codeday-2024",
      "description": "911 Police Suite Team Website for police department to deploy, track and add units aswell for society to track help",
      "readme": "This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `pages/index.js`. The page auto-updates as you edit the file.\n\n[API routes](https://nextjs.org/docs/api-routes/introduction) can be accessed on [http://localhost:3000/api/hello](http://localhost:3000/api/hello). This endpoint can be edited in `pages/api/hello.js`.\n\nThe `pages/api` directory is mapped to `/api/*`. Files in this directory are treated as [API routes](https://nextjs.org/docs/api-routes/introduction) instead of React pages.\n\nThis project uses [`next/font`](https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details.\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/codeday-2024",
      "commitCount": 12,
      "latestCommitDate": "2024-07-14T12:58:23Z",
      "topics": [
        "hackathon-project",
        "website-development"
      ]
    },
    {
      "title": "OneAI",
      "description": "Desktop Application for generating readme file for your projects working with different AI APIs at single click using Electron",
      "readme": "![image](https://github.com/somwrks/OneAI/assets/85481905/f6e32dd3-f85f-4acc-9614-742af14aa5ad)\n![image](https://github.com/somwrks/OneAI/assets/85481905/e95755fd-cc98-4dbd-bdbb-bc7c478d0595)\n\n![image](https://github.com/somwrks/OneAI/assets/85481905/2bccd5d8-dd0f-4e7c-8e5f-a414d31ccc5a)\n![image](https://github.com/somwrks/OneAI/assets/85481905/6fffb256-b5fe-4523-aa23-fc07d2c297b6)\n![image](https://github.com/somwrks/OneAI/assets/85481905/db18e542-0c62-415c-8baa-b45ca2109bca)\n\n\n# OneAI\n\nDesktop Application for generating readme file for your projects working with different AI APIs at single click using Electron\n\n\n## Contribute\n\n1. Ensure all dependencies are installed. You can install all dependencies by running `npm install` in the project's root directory.\n2. Create a duplicate file of `.env copy.local` and rename it to .env.local\n3. Enter your API keys and base url into the env file.\n4. Build the application by running `npm run build` in the project's root directory.\n5. Launch the application by running `npm run dev` in the project's root directory.\n\n## Overview\n\nThis project is a desktop application built using **NextronJS** and **TailwindCSS**, allowing users to generate professional README files for their projects with the help of various AI models such as OpenAI, Llama, Gemini, Claude, Perplexity, and more. Users can select their preferred AI model, provide their API keys, specify the project directory, and choose from a range of README templates. The AI will then generate a comprehensive README file based on the user's selections. Users can conveniently save the generated README file to their computer.\n\n\n## Dependencies\n\nThe project requires the following dependencies:\n\n- NextronJS\n- ElectronJS\n- TailwindCSS\n- Openai API\n- Gemini API\n- Claude API\n- Llama API\n- Perplexity API \n\n\n## Usage\n\n1. Ensure all dependencies are installed. You can install all dependencies by running `npm install` in the project's root directory.\n2. Build the application by running `npm run build` in the project's root directory.\n3. Launch the application by running `npm run start` in the project's root directory. \n\nYou will be greeted with the OneAI application. \n4. Provide your API keys for various AI models like OpenAI, Llama, Gemini, Claude, Perplexity.\n5. Select the desired template for your README file.\n6. Provide the directory to your project.\n7. OneAI will generate the README file based on your selections and save it to your computer. You can edit and customize the generated README file as needed. \n\n\n## Code Structure\n\nThe key components of the code include:\n\n- **UI:** The application's user interface is built using `NextronJS`, which leverages the power of `Next.js` for building web applications and integrates it with `ElectronJS` to create a native desktop application. The UI is styled using `TailwindCSS` for a modern and customizable look.\n- **AI Model Integration:**  The project seamlessly integrates with various leading AI models:\n    - **OpenAI API:** Users can leverage OpenAI's powerful models like GPT-3 and GPT-4 for advanced text generation and understanding.\n    - **Gemini API:** Access to Google's Gemini family of models, offering state-of-the-art performance for a range of tasks.\n    - **Claude API:** Utilize Anthropic's Claude, known for its safety and reliability in text generation and interaction.\n    - **Llama API:** Integrate with Meta's Llama models for large language model capabilities.\n    - **Perplexity API:** Connect to Perplexity AI, a powerful search engine and language model for information retrieval.\n- **API Key Management:**  Users can securely input their API keys for each model, allowing them to access and utilize the AI models within the application.\n- **Project Directory Integration:** The application enables users to provide the directory path to their projects, facilitating the automatic generation of READMEs based on the project's content and structure.\n- **Readme Template Selection:** Users can choose from a collection of pre-defined Readme templates, ensuring their Readme files adhere to best practices and conventions.\n- **Readme Generation:** The application leverages the selected AI model to automatically generate a high-quality Readme file based on the user's project directory and chosen template.\n- **Readme File Saving:** Users can conveniently save the generated Readme file to their computer for easy access and sharing. \n\n## Folder Structure\n\n- `.git`: Contains version control information for the project.\n- `electron-builder.yml`: Configuration file for Electron Builder, used to package the application for different platforms.\n- `main`: Contains the main process code for the Electron application.\n    - `background.ts`: Handles background tasks and communication with the renderer process.\n    - `helpers`: Contains helper functions for creating windows and managing the application.\n        - `create-window.ts`: Creates the main application window.\n        - `index.ts`: Main helper functions file.\n    - `preload.ts`: Preload script injected into the renderer process to provide additional functionalities.\n- `package-lock.json`: Contains information about the project's dependencies and their versions.\n- `package.json`: The project's manifest file, defining dependencies, scripts, and other configuration.\n- `README.md`: This file, providing information about the project.\n- `renderer`: Contains the frontend code built with Next.js, Electron, and TailwindCSS.\n    - `.env.local`: Environment variables specific to the development environment.\n    - `components`: Contains reusable UI components.\n        - `Chat.tsx`: Component for displaying and managing the chat interface.\n    - `middleware.ts`: Middleware functions to handle requests and responses.\n    - `next-env.d.ts`: Type definitions for Next.js environment variables.\n    - `next.config.js`: Configuration file for Next.js, defining build settings and optimizations.\n    - `pages`: Contains the application's pages.\n        - `api`: Contains API routes for handling file operations and interacting with AI models.\n            - `readfile.ts`: Route for reading files from the user's project directory.\n            - `savefile.ts`: Route for saving the generated README file.\n            - `[model].ts`: Route for interacting with specific AI models based on the provided API keys.\n        - `home.tsx`: The main landing page of the application.\n        - `_app.tsx`: The application's main component.\n    - `postcss.config.js`: Configuration file for PostCSS, defining styling rules and plugins.\n    - `preload.d.ts`: Type definitions for the preload script.\n    - `public`: Contains static assets for the application.\n        - `images`: Contains images for the application.\n            - `logo.png`: The application's logo.\n        - `template.json`: Contains pre-defined README templates for users to select from.\n    - `styles`: Contains global styles for the application.\n        - `globals.css`: Global CSS styles.\n    - `tailwind.config.js`: Configuration file for TailwindCSS, defining the styling framework.\n    - `tsconfig.json`: TypeScript configuration file, defining compiler settings.\n- `resources`: Contains icons for the application.\n    - `icon.icns`: Icon for macOS.\n    - `icon.ico`: Icon for Windows.\n- `tsconfig.json`: TypeScript configuration file for the main process. \n\n\n## License\n\nThis project is licensed under the [MIT License](https://opensource.org/licenses/MIT). Feel free to modify and distribute the code as per the terms of the license. \n\n\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/OneAI",
      "commitCount": 71,
      "latestCommitDate": "2024-07-04T06:44:50Z",
      "topics": [
        "artificial-intelligence",
        "desktop-development"
      ]
    },
    {
      "title": "Masked-Face-Detection",
      "description": "Deep Learning Project for analyzing various images and predicting masked faces according to requirements",
      "readme": "\n\n# Face Mask Detection\n\nThis project focuses on implementing a deep learning model for face mask detection using the VGG16 pre-trained model and transfer learning. The model is trained on a dataset of images with and without face masks.\n\n## Overview\n\nThe project consists of the following components:\n\n1. `main.ipynb`: This file contains the main Python code for loading the dataset, preprocessing data, building and training the model, and performing real-time face mask detection.\n\n\n![Untitled video - Made with Clipchamp](https://github.com/somwrks/Masked-Face-Detection/assets/85481905/3a0591e5-5e0b-4ea1-a676-9ca40f2c23c1)\n\n![image](https://github.com/somwrks/Masked-Face-Detection/assets/85481905/35cbe90d-34bf-4718-9890-ae0717a07e84)\n\n![image](https://github.com/somwrks/Masked-Face-Detection/assets/85481905/62de7f6e-5217-4c52-b9fd-4363567ae737)\n\n\n## Dependencies\n\nThe project requires the following dependencies:\n\n- NumPy\n- Pandas\n- OpenCV\n- Keras\n- scikit-learn\n- TensorFlow (for Keras)\n\n## Usage\n\n1. Ensure all dependencies are installed.\n2. Run the `paste.txt` file in a Python environment.\n\n## Code Structure\n\nThe key components of the code include:\n\n- Loading and preprocessing the dataset\n- Building the VGG16 model and adding a custom dense layer\n- Transfer learning by freezing the pre-trained layers\n- Training the model on the dataset\n- Real-time face mask detection using OpenCV\n\n## Folder Structure\n\n- `train`: Contains the training dataset with two folders: `with_mask` and `without_mask`.\n\n## How to Contribute\n\nContributions to this project are welcome! If you have any suggestions, improvements, or bug fixes, please feel free to create issues or pull requests.\n\n## License\n\nThis project is licensed under the [MIT License](https://opensource.org/licenses/MIT). Feel free to modify and distribute the code as per the terms of the license.\n\n\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Masked-Face-Detection",
      "commitCount": 11,
      "latestCommitDate": "2024-06-23T18:59:07Z",
      "topics": [
        "artificial-intelligence"
      ]
    },
    {
      "title": "AR-Quiz",
      "description": "Advanced Machine Learning WebApp for analyzing face detection and predicting cheating behaviors according to requirements",
      "readme": "# Test Taking Website with Facial Recognition\n\n## Overview\nThis project is a test-taking website with facial recognition to detect cheating behaviors. Users will be required to keep looking into the camera during the test, and if they look away or exhibit cheating behaviors, the test will be terminated. The project uses Next.js for the frontend, Flask for the backend, OpenCV/Mediapipe for facial recognition, and Supabase/Prisma for the database.\n\n![image](https://github.com/somwrks/AR-Quiz/assets/85481905/49d4022f-0748-4e67-b8ab-895e30e82f54)\n\n\n![Untitledvideo-MadewithClipchamp-ezgif com-video-to-gif-converter](https://github.com/somwrks/AR-Quiz/assets/85481905/2b2223b8-4cb3-4917-8cff-a2004ca0cd23)\n\n![image](https://github.com/somwrks/AR-Quiz/assets/85481905/12c42e15-2984-43f0-a68e-7542dd6e71ae)\n\n\n## Tech Stack\n- Frontend:\n  - Next.js\n  - Tailwind CSS\n  - React Webcam\n- Backend:\n  - Flask\n  - OpenCV/Mediapipe\n  - Prisma (interacting with Supabase)\n- Database:\n  - Supabase (PostgreSQL)\n\n## Getting Started\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/somwrks/AR-Quiz.git\n   cd AR-Quiz\n2. Set up the backend:\n    Create a virtual environment (optional but recommended)\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # Activate the virtual environment\n    ```\n    Install dependencies\n    ```bash\n    pip install -r requirements.txt\n    ```\n    Set up environment variables:\n    Create a .env file based on .env.example and fill in the necessary credentials\n    Run the Flask app:\n    ```bash\n    flask run\n    ```\n3. Set up the frontend:\n   Navigate to the frontend directory:\n   ```bash\n   cd frontend\n   ```\n   Install dependencies:\n   ```bash\n   npm install\n   ```\n   Start the Next.js development server:\n   ```bash\n   npm run dev\n   ```\n   Start the Backend Flask server:\n   ```bash\n   npm run start-backend\n   ```\n4. Access the website:\n   Open your browser and go to http://localhost:3000 to access the test-taking website\n\n## Project Structure\n- `backend/`: Contains Flask backend code, including facial recognition logic.\n- `frontend/`: Contains Next.js frontend code, including React Webcam integration.\n- `database/`: Contains SQL scripts for setting up the database schema.\n- `docs/`: Contains project documentation, including this README.md.\n\n## Contributing\nFeel free to contribute to this project by submitting pull requests. Make sure to follow the coding guidelines and provide clear commit messages.\n\n## License\nThis project is licensed under the MIT License.\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/AR-Quiz",
      "commitCount": 12,
      "latestCommitDate": "2024-06-23T18:48:25Z",
      "topics": [
        "artificial-intelligence",
        "website-development"
      ]
    },
    {
      "title": "pearai-app",
      "description": "An Open Sourced AI-Powered Text Editor. A fork of VSCode and Continue.",
      "readme": "# Pear: The Best AI-Powered Code Editor\n\nSupercharge your development with an editor designed for less coding, using AI. PearAI is forked from VSCode and Continue, and aims to reduce the time from ideation to conception for your product development by achieving the most seamless integration with AI.\n\nThis repository serves as the primary application for PearAI, with most functionalities housed within the extension/pearai folder. We recommend focusing your work within this submodule by cloning it from https://github.com/trypear/pearai-app/.\n\nTo download the full product visit our homepage at https://trypear.ai.\n\n# Prerequisites\n\nPearAI currently only supports OpenAI at this time. To obtain an OpenAI API key go to [platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys).\n\n## Features\n* **Knows your code** - Pear has context on your database so you can ask questions about your code.\n* **Auto-implement features** - Ask for a feature and receive a PR implementing it.\n* **UI/UX focused** - Pear puts the user experience first, making developing as seamless as possible.\n* **Never start from scratch** - Pear comes with high-quality templates and boilerplate code for any type of project.\n* **Batteries included** - Pear comes fully setup with shortcuts and terminal plugins used to supercharge development productivity.\n* **Familiar feel** - Pear is a fork of VSCode, so you can pick up exactly where you left off.\n\n## Roadmap\n\nOur [Master Document Roadmap](https://docs.google.com/document/d/14jusGNbGRPT8X6GgEDbP1iab5q4X7_y-eFXK7Ky57IQ/edit) contains the Pear AI roadmap, focusing on the teams development journey, and the community that surrounds Pear AI.\n\n## Contributing\n\nRead our to learn about our development process, how to propose bugfixes and improvements, and how to build and test your changes.\n\nWe welcome contributions from the community! Whether you're fixing a bug, improving the documentation, or adding a new feature, we appreciate your help in making PearAI better. To help you get your feet wet and become familiar with our contribution process, we have a list of [good first issues](https://github.com/trypear/pearai-app/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) that contains things with a relatively limited scope. This is a great place to get started!\n\n**Please review our [Contributing Guide](CONTRIBUTING.md) to get started.** For any questions, join the [PearAI Discord](https://discord.gg/7QMraJUsQt)!\n\nCurious about our stack?\n- PearAI is in TypeScript/Electron.js\n- PearAI landing page is Next.js/React with Supabase auth (TailwindCSS + Shadcn)\n- PearAI backend is a Python Flask server with Supabase database\n- Logging/Telemetry is done with Axiom\n\n## License\nPear OSS is licensed under the Apache 2.0 license. See the LICENSE file for details.\n\n## Contact\nFor any questions or issues, please open an issue or reach out in the PearAI [Discord](https://discord.gg/7QMraJUsQt).\n\n## Acknowledgements\n\nThanks to these wonderful people who have contributed to this project:\n- [Nang](https://github.com/nang-dev)\n- [FryingPannnn](https://github.com/Fryingpannn)\n- [ItWasEnder](https://github.com/ItWasEnder)\n- [Gedeondoescode](https://github.com/gedeondoescode)\n\nFeel free to join them and contribute!\n\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/pearai-app",
      "commitCount": 121245,
      "latestCommitDate": "2024-06-17T18:47:56Z",
      "topics": [
        "artificial-intelligence",
        "desktop-development"
      ]
    },
    {
      "title": "SafeSentry",
      "description": "TechXlerate Hackathon 2023",
      "readme": "# SafeSentry\n\nSafeSentry is a mobile app designed to enhance safety and security for high school students. Built using React Native, TailwindCSS, and Firebase, it offers a range of features to help students proactively address potential gun violence threats. Students can register using their student ID or school email, allowing them to access the app's functionalities within their school ecosystem.  SafeSentry enables students to anonymously file evidence-based incident reports, detailing suspicious activities or potential threats. This feature provides a valuable tool for school authorities to investigate and potentially prevent incidents.  The app also includes an interactive map that displays incident reports, safety exits, and designated shelters, empowering students with crucial knowledge in emergencies.  Furthermore, SafeSentry offers an Emergency Aid feature that allows students to quickly alert local authorities with a single click, potentially saving lives in critical situations. School administrators and authorities can utilize the timeline location feature to track student accounts during emergencies.  SafeSentry's innovative features aim to significantly reduce casualties in the event of a school shooting or even prevent such incidents altogether. \n\n\n## Overview\n\nSafeSentry is a mobile application built using React Native, TailwindCSS, and Firebase.  It's designed to provide high school students with a variety of personal security and safety features to help prevent gun violence.  Key features include a log-in/authentication system where students can register using their student ID or other information, a secure incident reporting system allowing students to report suspicious activity or potential threats, and an interactive map showing locations of incident reports, safety exits, and shelters. The app also has an emergency aid feature that allows students to quickly alert local authorities with a single click, and a timeline location feature that allows administrators and authorities to track students during emergencies.  SafeSentry is equipped with innovative features to help reduce casualties in school shootings and potentially prevent them entirely. \n\n\n## Dependencies\n\nThis project is a mobile application built using React Native, TailwindCSS, and Firebase, designed to improve school safety and potentially prevent gun violence. SafeSentry offers a range of features for high school students, including:\n\n- **Secure Login/Authentication:** Students can register with the app using their student ID or school email.\n- **Incident Reporting:** Students can file evidence-based reports about suspicious activity or potential threats.\n- **Interactive Map:** Displays locations of incident reports, safety exits, and shelters, providing critical information in emergencies.\n- **Emergency Aid:** Students can quickly alert local authorities with a single button click during an active threat.\n- **Timeline Location Tracking:** School administrators and authorities can track students' locations during emergencies.\n\nSafeSentry aims to empower students and provide tools to promote a safer learning environment.\n\n\n## Usage\n\n1. Ensure all dependencies are installed. You can install all dependencies by running `npm install` in the project's root directory.\n2. Build the application by running `npm run build` in the project's root directory.\n3. Launch the application by running `npm run start` in the project's root directory. \n4. Navigate through the application using the provided navigation. \n5. You can view details on the application by accessing the `Profile` and `Reports` pages. \n6. You can interact with the Emergency Aid, Safety Map, and Report features to experience the key components of the application.\n7. Use the provided log-in features to simulate real-world interactions.\n8. You can explore the provided `reports.json` and `aids.json` files to get an understanding of the app's structure.\n9. The application is designed to provide a user-friendly interface and to showcase the functionality of the app.\n10. You can modify and customize the UI as needed based on your preferences. \n\n\n## Code Structure\n\nThe key components of the code include:\n\n- **UI:** The application's user interface is built using `React Native`, which leverages the power of JavaScript to create a cross-platform mobile application. The UI is styled using `TailwindCSS` for a modern and customizable look. \n- **Authentication:** The application utilizes `Firebase` for secure user login and authentication. Users can register and log in using their student ID or school email.\n- **Incident Reporting:** The `Reports.js` and `ReportDetails.js` components allow students to file evidence-based incident reports about suspicious activities or potential threats. These reports can help prevent future incidents. \n- **Interactive Map:** The `Map.js` component provides an interactive map that displays locations of incident reports, safety exits, and safety shelters. This feature enhances situational awareness and provides vital information during emergencies. \n- **Emergency Aid:** The `Emergency.js` component features an Emergency Aid button that, with a single click, alerts local authorities. Students can quickly access this feature when faced with a real threat. \n- **Timeline Location Tracking:** School administrators and authorities can use the timeline location feature to track students' locations through their accounts, providing crucial information during emergencies.\n- **Notifications:** The `Notifications.js` component manages and displays important alerts and updates to users. \n- **Other Components:**  The project includes various other components such as `Add.js`, `Carousel.js`, `CategorySlider.js`, `Navbar.js`, `Onboarding.js`, `OnboardingData.js`, `Home.js`, `LoginScreen.js`, `Profile.js`, `Search.js`, and `style.js`, which contribute to the application's functionality and user experience. \n- **Data Storage:** The project utilizes `aids.json` and `reports.json` files to store relevant data for incident reports, safety resources, and user information.\n\n\n## Folder Structure\n\n- `.git`: Contains version control information for the project.\n- `Add.js`: Component for adding new report information.\n- `aids.json`: Contains data for emergency aid options.\n- `App.js`: The main application entry point.\n- `app.json`: Configuration file for the React Native application.\n- `babel.config.js`: Configuration file for Babel, defining transpilation rules.\n- `Carousel.js`: Component for displaying a carousel of images or content.\n- `CategorySlider.js`: Component for displaying a slider of categories.\n- `Navbar.js`: Component for displaying the navigation bar.\n- `Onboarding.js`: Component for displaying the onboarding screens.\n- `OnboardingData.js`: Data for the onboarding screens.\n- `Reports.js`: Component for displaying and managing reports.\n- `Emergency.js`: Component for handling emergency situations.\n- `Home.js`: Component for the home screen.\n- `LoginScreen.js`: Component for the login screen.\n- `Map.js`: Component for displaying the interactive map.\n- `Notifications.js`: Component for managing notifications.\n- `onboard.js`:  Component for displaying the onboarding screens.\n- `package-lock.json`: Contains information about the project's dependencies and their versions.\n- `package.json`: The project's manifest file, defining dependencies, scripts, and other configuration.\n- `Profile.js`: Component for displaying the user profile.\n- `ReportDetails.js`: Component for displaying details of a specific report.\n- `reports.json`: Contains data for the reports.\n- `Search.js`: Component for searching reports.\n- `style.js`: Contains global styles for the application.\n- `UserContext.js`: Context for managing user authentication and data.\n- `yarn.lock`:  Lockfile for Yarn, managing dependency versions. \n\n\n## License\n\nThis project is licensed under the [MIT License](https://opensource.org/licenses/MIT). Feel free to modify and distribute the code as per the terms of the license. \n\n\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/SafeSentry",
      "commitCount": 16,
      "latestCommitDate": "2024-06-04T08:29:31Z",
      "topics": [
        "app-development",
        "hackathon-project"
      ]
    },
    {
      "title": "EduMon",
      "description": "Connecting Schools, Building Communities",
      "readme": "# Edumon\n\nEdumon is an AI-powered social networking platform built with Next.js, TailwindCSS, Clerk API, OpenAI API, and MongoDB. It's designed to connect schools, students, and clubs within a dynamic and engaging environment.  Schools can create profiles showcasing their unique features, establish clubs for students to join, and post announcements to keep everyone informed. Students can participate in clubs and activities, stay updated on school news, and connect with others through the built-in social networking features. Edumon allows schools to share their stories, connect with their communities, and foster a vibrant learning ecosystem.\n\n\n## Dependencies\n\nThis project is a social networking platform for schools powered by AI. It allows schools to create profiles, showcase their activities, and manage clubs. Students can participate in clubs, view announcements, and engage in social networking features. The platform is built using Next.js for the frontend, TailwindCSS for styling, and ClerkAPI for user authentication. Openai API is integrated for generating content and enhancing user interactions. The backend utilizes MongoDB for data storage. \n\n\n## Usage\n\n1. Ensure all dependencies are installed. You can install all dependencies by running `npm install` in the project's root directory.\n2. Start the development server by running `npm run dev` in the project's root directory.\n3. This will open the application in your browser, and you can start exploring and interacting with Edumon.\n4. The application utilizes various third-party APIs for its features, such as OpenAI API for generating text content, Clerk API for user authentication, and MongoDB for data storage. You can find the necessary API keys and configurations in the `mongodb.js` and `next.config.js` files.\n5. The Edumon application is built using a combination of Next.js for the frontend, Tailwind CSS for styling, and MongoDB for database management. The core functionality of the platform revolves around connecting schools and students through clubs, activities, and social interactions.\n6. The `schools.json`, `users.json`, and `clubs.json` files represent the database structure for the application, storing information about schools, users, and clubs. \n7. The frontend components are organized into various files like `Home.jsx`, `Navbar.jsx`, `Sidebar.jsx`, `Welcome.jsx`, and `Details.jsx`, each responsible for a specific aspect of the user interface. \n8. The backend logic is implemented in files like `index.js`, `[slug].js`, `events.js`, and `explore.js`, handling user requests and interactions with the database. \n9. Edumon provides a platform where schools can create profiles, set up clubs, and manage their activities. Students can join clubs, participate in events, and interact with other students from their school and other schools.\n10. The application aims to foster a sense of community and provide a centralized space for school-related information, announcements, and interactions. \n\n\n## Code Structure\n\nThe key components of the code include:\n- **UI:** The application's user interface is built using `NextJS` and styled with `TailwindCSS` for a modern and responsive look.\n- **Database:** The project utilizes `MongoDB` to store and manage data related to schools, clubs, users, posts, events, and other relevant information.\n- **Authentication:**  `ClerkAPI` handles user authentication and authorization, enabling secure login and user management.\n- **Social Features:** The platform includes features like posting, liking, commenting, and interacting with other users within the school community.\n- **School Management:** Schools can create their profiles, showcase their details, manage clubs and activities, and post announcements within the platform.\n- **Club Management:**  Students can join clubs, participate in activities, and access information related to their clubs.\n- **AI Integration:** The project utilizes `OpenaiAPI` for features like generating text, understanding context, and potentially enhancing other functionalities, although specific implementations are not provided in the file list. \n- **API Requests:**  The `fetchData.js` file is likely responsible for handling API requests to fetch and manage data from the database and other services.\n- **Components:** The project includes various components such as `Navbar.jsx`, `Sidebar.jsx`, `Home.jsx`, and others to create the user interface and manage application flow. \n- **Server-Side Rendering:** `NextJS` enables server-side rendering and dynamic data fetching, improving SEO and performance. \n\n\n\n## Folder Structure\n\n- `CustomLoader.jsx`:  Component for displaying loading animations.\n- `Details.jsx`: Component for displaying detailed information.\n- `fetchData.js`: Contains functions for fetching data from various sources like MongoDB or external APIs.\n- `Home.jsx`: Component for the homepage.\n- `Navbar.jsx`: Component for the navigation bar.\n- `Sidebar.jsx`: Component for the sidebar navigation.\n- `Welcome.jsx`: Component for the welcome screen.\n- `errors.txt`: A file for storing errors encountered during development.\n- `mongodb.js`: Contains functions for connecting to and interacting with the MongoDB database.\n- `next.config.js`: Configuration file for Next.js, defining build settings and optimizations.\n- `package-lock.json`:  Contains information about the project's dependencies and their versions.\n- `package.json`:  The project's manifest file, defining dependencies, scripts, and other configuration.\n- `README.md`: This file, providing information about the project.\n- `comment.js`: File containing functions related to comments.\n- `hello.js`: File containing functions related to greetings.\n- `join.js`: File containing functions related to joining clubs or communities.\n- `leave.js`: File containing functions related to leaving clubs or communities.\n- `like.js`: File containing functions related to liking posts or content.\n- `register.js`: File containing functions related to user registration.\n- `[type].js`: A generic file for specific types of content or functionalities.\n- `index.js`: The main file for different pages and functionalities.\n- `[slug].js`: A file for handling individual pages or content based on a unique slug.\n- `clubs.js`: File containing functions related to clubs and clubs-related data.\n- `clubs.json`: A JSON file storing data about clubs.\n- `competetions.json`:  A JSON file storing data about competitions.\n- `notifications.json`: A JSON file storing data about notifications.\n- `posts.json`:  A JSON file storing data about posts.\n- `schools.json`: A JSON file storing data about schools.\n- `users.json`: A JSON file storing data about users.\n- `webinars.json`: A JSON file storing data about webinars.\n- `workshops.json`: A JSON file storing data about workshops.\n- `globals.css`: Contains global CSS styles.\n- `tailwind.config.js`: Configuration file for TailwindCSS, defining the styling framework. \n\n\n## License\n\nThis project is licensed under the [MIT License](https://opensource.org/licenses/MIT). Feel free to modify and distribute the code as per the terms of the license. \n\n\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/EduMon",
      "commitCount": 31,
      "latestCommitDate": "2024-06-04T08:25:50Z",
      "topics": [
        "hackathon-project",
        "website-development"
      ]
    },
    {
      "title": "DSA",
      "description": "All my DSA Course Notes and Notebooks",
      "readme": "All my Data Structures Notes and Code throughout all courses\n\n![image](https://github.com/somwrks/Data-Structures/assets/85481905/93a9c815-52ad-48f7-81dc-d6ce693ccacb)\n\n![image](https://github.com/somwrks/Data-Structures/assets/85481905/de1ad3b8-edbf-4559-ad49-9bd174eb3a0a)\n\n![image](https://github.com/somwrks/Data-Structures/assets/85481905/1e3d7064-7416-443c-849c-077f6e059dbc)\n![image](https://github.com/somwrks/Data-Structures/assets/85481905/adbea1f3-226c-4750-a3d0-788b91defb4a)\n![image](https://github.com/somwrks/Data-Structures/assets/85481905/0b802837-45b9-4dc2-8b97-e7edf39f3035)\n![image](https://github.com/somwrks/Data-Structures/assets/85481905/bb162cd1-70e5-418c-9d8a-acb027946656)\n![image](https://github.com/somwrks/Data-Structures/assets/85481905/c7421c3d-9c65-4212-a96b-04da6ba3de81)\n![image](https://github.com/somwrks/Data-Structures/assets/85481905/265721c6-e907-4a42-bff1-839f2305cf09)\n\n![image](https://github.com/somwrks/Data-Structures/assets/85481905/7728f6b9-13a8-4f66-b6c9-8e820f99515d)\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/DSA",
      "commitCount": 10,
      "latestCommitDate": "2024-05-19T12:03:22Z",
      "topics": [
        "notes"
      ]
    },
    {
      "title": "Image-Classifier",
      "description": "Deep Learning Project for analyzing images and predicting emotions according to requirements",
      "readme": "# Image Classifier\n\nThis project focuses on building an image classifier using Convolutional Neural Networks (CNN) to classify images into two categories: 'happy' and 'sad'. The project is implemented using TensorFlow and Keras.\n\n## Overview\n\nThe project consists of the following components:\n\n1. `main.ipynb`: This Jupyter Notebook file contains the main codebase for building, training, and evaluating the image classifier model.\n\n## Dataset\n\nThe dataset used in this project is located in the `data` directory, which contains two subdirectories:\n\n- `happy`: Contains images of happy people.\n- `sad`: Contains images of sad people.\n\n## Dependencies\n\nThe project requires the following dependencies:\n\n- TensorFlow (version >= 2.x)\n- Keras\n- OpenCV\n- Matplotlib\n- NumPy\n- Pandas\n\n## Usage\n\n1. Ensure all dependencies are installed.\n2. Open and run the `main.ipynb` notebook in a Jupyter environment.\n3. Follow the instructions and code comments within the notebook to preprocess the data, build the model, train, evaluate, and make predictions.\n\n## Code Structure\n\nThe key components of the code include:\n\n- Data preprocessing and loading using OpenCV and TensorFlow Datasets.\n- Building the CNN model using Keras.\n- Training the model using TensorFlow's `fit` method.\n- Monitoring the training progress using TensorBoard callbacks.\n- Evaluating the model's performance using metrics like accuracy, precision, and recall.\n- Making predictions on new images.\n- Saving and loading the trained model.\n\n## Folder Structure\n\n- `data`: Contains the image dataset.\n  - `happy`: Images of happy people.\n  - `sad`: Images of sad people.\n- `logs`: Directory for storing TensorBoard logs during training.\n- `models`: Directory for storing trained models.\n  - `imageclassifier.h5`: The trained image classifier model.\n\n## How to Contribute\n\nContributions to this project are welcome! If you have any suggestions, improvements, or bug fixes, please feel free to create issues or pull requests.\n\n## License\n\nThis project is licensed under the MIT License. Feel free to modify and distribute the code as per the terms of the license.\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Image-Classifier",
      "commitCount": 5,
      "latestCommitDate": "2024-05-18T08:19:21Z",
      "topics": [
        "artificial-intelligence"
      ]
    },
    {
      "title": "Sentimental-Analysis",
      "description": "Full Stack Machine Learning WebApp for analyzing large dataset of tweets and predicting text emotions according to requirements",
      "readme": "# Sentiment Analysis App\n\nThis is a Next.js application that performs sentiment analysis on text data using a TensorFlow.js model. The application is built with TypeScript and utilizes the TensorFlow.js library for training and evaluating the machine learning model.\n\n## Overview\n\nThe project consists of the following components:\n\n1. `pages/index.tsx`: The main page that allows users to input text and receive the sentiment analysis result.\n2. `ml/sentimentAnalysis.ts`: This module contains the core logic for training the sentiment analysis model and performing predictions.\n\n## Dependencies\n\nThe project requires the following dependencies:\n\n- Next.js\n- TensorFlow.js\n- React\n- TypeScript\n\n## Setup\n\n1. Clone the repository:\n\n```bash\ngit clone https://github.com/your-repo/sentiment-analysis-app.git\n```\n2. Installing dependencies:\n   ```bash\n   cd sentiment-analysis-app\n   npm install\n   ```\n4. Start the development server:\n   ```bash\n   npm run dev\n   ```\n   The application will be accessible at http://localhost:3000.\n\n## Usage\n  \n  1. Open the application in your web browser.\n  2. Enter some text in the input field.\n  3. Click the \"Analyze Sentiment\" button.\n  4. The sentiment analysis result (positive, negative, or neutral) will be displayed.\n\n## Code Structure\n\n- pages/index.tsx: This file contains the main page component, which includes the input field and displays the sentiment analysis result.\n\n- ml/sentimentAnalysis.ts: This module is responsible for training the sentiment analysis model and performing predictions. It utilizes the TensorFlow.js library for machine learning tasks.\n\n- public/dataset/: This directory contains the CSV files used for training and testing the sentiment analysis model.\n\n## Deployment\n\n  The Next.js application can be deployed to various hosting platforms, such as Vercel, Netlify, or a custom server.\n\n## License\n\n  This project is licensed under the MIT License.\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Sentimental-Analysis",
      "commitCount": 7,
      "latestCommitDate": "2024-05-18T08:09:01Z",
      "topics": [
        "artificial-intelligence"
      ]
    },
    {
      "title": "Stock-Predictions",
      "description": "Machine Learning Project for analyzing stocks and predicting bullish/bearish according to requirements",
      "readme": "# Tesla Stock Price Analysis and Prediction\n\nThis project focuses on analyzing and predicting the stock prices of Tesla, Inc. using historical stock data. The project utilizes Python, Pandas, Matplotlib, and Scikit-learn to perform data analysis, visualization, and machine learning modeling.\n\n## Overview\n\nThe project consists of the following components:\n\n1. `prediction.ipynb`: This Jupyter Notebook file contains the main codebase for data loading, exploratory data analysis, data preprocessing, model training, and evaluation.\n\n## Dependencies\n\nThe project requires the following dependencies:\n\n- Python (version >= 3.6)\n- Pandas\n- NumPy\n- Matplotlib\n- Scikit-learn\n- Plotly (for interactive visualizations)\n\n## Data\n\nThe project uses the `tesla.csv` dataset, which contains historical stock data for Tesla, Inc., including the date, open, high, low, close, adjusted close, and volume.\n\n## Usage\n\n1. Ensure all dependencies are installed.\n2. Open and run the `prediction.ipynb` notebook in a Jupyter environment.\n3. Follow the instructions and code comments within the notebook to perform data exploration, preprocessing, and model training and evaluation.\n\n## Code Structure\n\nThe key components of the code include:\n\n- Data loading and exploration\n- Data visualization (line plots, box plots)\n- Data preprocessing (scaling, splitting into train and test sets)\n- Model training using Linear Regression\n- Model evaluation (R-squared, Mean Squared Error, Root Mean Squared Error)\n\n## Files\n\n- `tesla.csv`: The dataset containing historical stock data for Tesla, Inc.\n- `Google_test_data.csv`: Test data for Google (if applicable)\n- `Google_train_data.csv`: Training data for Google (if applicable)\n- `google.ipynb`: Jupyter Notebook for Google stock analysis (if applicable)\n\n## Folder Structure\n\n- `images`: Generated visualizations during data exploration and analysis (if applicable).\n\n## How to Contribute\n\nContributions to this project are welcome! If you have any suggestions, improvements, or bug fixes, please feel free to create issues or pull requests.\n\n## License\n```bash\nThis project is licensed under the MIT License. Feel free to modify and distribute the code as per the terms of the license.\n```\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Stock-Predictions",
      "commitCount": 2,
      "latestCommitDate": "2024-05-18T08:05:06Z",
      "topics": [
        "artificial-intelligence"
      ]
    },
    {
      "title": "Audio-Classifier",
      "description": "Deep Learning Project for analyzing different audio and predicting classifications according to requirements",
      "readme": "# Audio Classifier Project\n\nThis project implements an audio classifier using TensorFlow and Keras to classify audio clips as either capuchin bird calls or not.\n\n## Contents\n\n- `main.ipynb`: Jupyter Notebook containing the code for the audio classifier.\n- `data/`: Directory containing audio files for training and testing.\n- `results.csv`: CSV file containing the classification results.\n\n## Requirements\n\n- Python 3.x\n- TensorFlow\n- TensorFlow IO\n- Librosa\n- Matplotlib\n\n## Usage\n\n1. Clone the repository and download the dataset from kaggle:\n\n  Download data set from [Kaggle](https://www.kaggle.com/datasets/kenjee/z-by-hp-unlocked-challenge-3-signal-processing)\n  \n  \n  ```bash\n  git clone https://github.com/somwrks/Audio-Classifier.git\n  cd Audio-Classifier\n  ```\n\n2. Install the required dependencies:\n  \n  ```bash\n  pip install tensorflow tensorflow-io librosa matplotlib\n  ```\n\n3. Run the Jupyter Notebook main.ipynb to train the audio classifier and classify new audio files.\n\n4. After running the notebook, the classification results will be saved in results.csv.\n\n## Notes\n  \n  - The preprocess_mp3 function in the notebook is designed for WAV files. If using MP3 files, modify this function accordingly.\n  \n  - Monitor training/validation metrics during model training to ensure convergence.\n  \n  - Verify the contents of results.csv to ensure correct classification results are saved.\n\n## License\n\n  This project is licensed under the MIT License.\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Audio-Classifier",
      "commitCount": 7,
      "latestCommitDate": "2024-05-09T12:51:58Z",
      "topics": [
        "artificial-intelligence"
      ]
    },
    {
      "title": "QuotGen",
      "description": "Full Stack WebApp to generate inspirational quotes and download options",
      "readme": "# Quote Generator App\n\nThis project is a simple quote generator built using Next.js, TypeScript, and AWS Amplify. It fetches quotes from the ZenQuotes API and provides a user-friendly interface for users to discover and share inspirational quotes.\n\n## Features\n\n- Fetches quotes from the ZenQuotes API\n- Displays a dynamic background for an engaging user experience\n- Allows users to download quotes as images\n- Supports authentication for personalized experiences\n- Integrates CI/CD pipeline for seamless deployment\n\n## Technologies Used\n\n- Next.js\n- TypeScript\n- AWS Amplify\n- ZenQuotes API\n- Styled Components (for styling)\n\n## Getting Started\n\n1. Clone the repository to your local machine.\n2. Install dependencies using `npm install`.\n3. Set up your AWS Amplify project and configure authentication.\n4. Replace the ZenQuotes API key with your own key.\n5. Run the development server using `npm run dev`.\n6. Access the app at `http://localhost:3000`.\n\n## Folder Structure\n\n|-- pages/ # Next.js pages\n|-- components/ # React components\n|-- styles/ # CSS styles\n|-- public/ # Static assets\n|-- aws-exports.js # AWS Amplify configuration\n|-- next.config.js # Next.js configuration\n|-- tsconfig.json # TypeScript configuration\n|-- package.json # Project dependencies and scripts\n\n\n## Deployment\n\n1. Set up your AWS Amplify environment.\n2. Configure the CI/CD pipeline in AWS Amplify.\n3. Push changes to your GitHub repository to trigger deployments.\n\n## Contributing\n\nContributions are welcome! Feel free to open a pull request or create an issue for any suggestions or improvements.\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n\n## Acknowledgements\n\n- ZenQuotes API for providing inspirational quotes.\n- AWS Amplify for seamless deployment and authentication.\n- Next.js and TypeScript communities for excellent development tools.\n\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/QuotGen",
      "commitCount": 9,
      "latestCommitDate": "2024-04-27T23:18:29Z",
      "topics": [
        "website-development"
      ]
    },
    {
      "title": "HackUnited",
      "description": "United Hacks Hackathon WEbsite",
      "readme": "This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `pages/index.js`. The page auto-updates as you edit the file.\n\n[API routes](https://nextjs.org/docs/api-routes/introduction) can be accessed on [http://localhost:3000/api/hello](http://localhost:3000/api/hello). This endpoint can be edited in `pages/api/hello.js`.\n\nThe `pages/api` directory is mapped to `/api/*`. Files in this directory are treated as [API routes](https://nextjs.org/docs/api-routes/introduction) instead of React pages.\n\nThis project uses [`next/font`](https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details.\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/HackUnited",
      "commitCount": 53,
      "latestCommitDate": "2023-11-22T16:19:50Z",
      "topics": [
        "website-development"
      ]
    },
    {
      "title": "Autera",
      "description": "Building Bridges to Brighter Tomorrows",
      "readme": "This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `pages/index.js`. The page auto-updates as you edit the file.\n\n[API routes](https://nextjs.org/docs/api-routes/introduction) can be accessed on [http://localhost:3000/api/hello](http://localhost:3000/api/hello). This endpoint can be edited in `pages/api/hello.js`.\n\nThe `pages/api` directory is mapped to `/api/*`. Files in this directory are treated as [API routes](https://nextjs.org/docs/api-routes/introduction) instead of React pages.\n\nThis project uses [`next/font`](https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details.\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Autera",
      "commitCount": 7,
      "latestCommitDate": "2023-09-03T18:45:54Z",
      "topics": [
        "hackathon-project",
        "website-development"
      ]
    },
    {
      "title": "ecobliss",
      "description": "Environment Friendly Goods Store | Hackathon Citro Hacks '23 Submission",
      "readme": "## Only Preview works in production mode, Backend May not work.\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/ecobliss",
      "commitCount": 8,
      "latestCommitDate": "2023-07-16T20:47:30Z",
      "topics": [
        "hackathon-project",
        "website-development"
      ]
    },
    {
      "title": "Transformer-Neural-Network",
      "description": "Code Transformer neural network components piece by piece",
      "readme": "# Transformer-Neural-Network\nCode Transformer neural network components piece by piece. \n\n[Full Playlist](https://www.youtube.com/playlist?list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4)\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Transformer-Neural-Network",
      "commitCount": 17,
      "latestCommitDate": "2023-05-01T18:44:05Z",
      "topics": [
        "artificial-intelligence"
      ]
    },
    {
      "title": "SomFlix",
      "description": "Netflix Clone made using NextJS, Tailwind and Firebase",
      "readme": "# SomFlix | Netflix Clone\n\nNetflix Cloned functional and respnnsive website that fetches real time updated movies, has payment and login authentication.\n\n\n# Features\n- Login Authentication\n- Payment Authentication\n- Realtime Movies \n\n**Frontend** - NextJS and Tailwind\n**Backend** - NextJS\n**Database** - Firebase\n**Libraries** - Material Icons, React Icons and Rest API\n\n![image](https://user-images.githubusercontent.com/85481905/224714133-28b19d3d-cc30-41c6-9ba3-35a50411ae1b.png)\n![image](https://user-images.githubusercontent.com/85481905/224714255-3faeb9f5-a80f-494d-9488-7fbf93788806.png)\n![image](https://user-images.githubusercontent.com/85481905/224714338-38b478eb-4d43-49f3-8877-552cc10d483d.png)\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/SomFlix",
      "commitCount": 12,
      "latestCommitDate": "2023-03-13T13:23:37Z",
      "topics": [
        "website-development"
      ]
    },
    {
      "title": "Book-Recommendor",
      "description": "Book Recommender Website Made using NextJS and Open API",
      "readme": "# Book Recommender | ChatGPT Clone\n\nWebsite made using NextJS and OpenAI API that suggests books related to user's interests.\n\nhttps://user-images.githubusercontent.com/85481905/223776826-e5c25086-846f-4fbc-b079-bf30fb024b9c.mp4\n\nFrontend + Backend - NextJS\nOthers - OpenAPI\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Book-Recommendor",
      "commitCount": 46,
      "latestCommitDate": "2023-03-10T10:04:02Z",
      "topics": [
        "artificial-intelligence",
        "website-development"
      ]
    },
    {
      "title": "Books-of-Gold",
      "description": "Advanced E-Commerce Website Made using MERN Stack, Tailwind and Rest API",
      "readme": "# Books of Gold | E Commerce Store\nAdvanced and Updated Version of my old **E-Commerce Website**. It has both Admin and Client Side Dashboards, making it a potential for large-scale websites.\n\n![image](https://user-images.githubusercontent.com/85481905/212071161-83005597-f209-4575-9355-df98f38f8655.png)\n\n# Features\n\n- Large Scale Capability and Flexibility\n- Login Authentication\n- Payment Authentication\n\n**Frontend** - ReactJS, and Tailwind\n**Backend** - NodeJs, ExpressJs\n**Database** - MongoDB Cloud\n\n**Frontend Libraries** - Flowbite, Redux, npm overlay navbar, and Sass\n**Backend Libraries** - REST API\n\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Books-of-Gold",
      "commitCount": 14,
      "latestCommitDate": "2023-03-10T10:00:07Z",
      "topics": [
        "website-development"
      ]
    },
    {
      "title": "NewsMonke",
      "description": "News Website made using React, Bootstrap and NewsAPI",
      "readme": "# NewsMonke | News Website\nNewsMonke is a News website that has the functionality of fetching news from the API calls served by News API. Users can view content based on the different categories on a selection with dark and light modes.\n\nFrontend - React, Bootstrap\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/NewsMonke",
      "commitCount": 13,
      "latestCommitDate": "2023-02-16T08:52:50Z",
      "topics": [
        "website-development"
      ]
    },
    {
      "title": "Cross-that-mark-",
      "description": "Tic Tac Toe Web Game made using HTML, CSS and Javascript",
      "readme": "# Cross-that-mark-\nTic Tac Toe Web Game made using HTML, CSS and Javascript\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Cross-that-mark-",
      "commitCount": 3,
      "latestCommitDate": "2023-02-14T17:39:23Z",
      "topics": [
        "website-development"
      ]
    },
    {
      "title": "Wonder-Gallery-23",
      "description": "Hackathon Project for E-Summit'23 Codathon",
      "readme": "# Wonder's Gallery\n\n- My Submission for IIT Roorkee E-Summit'23 Codathon. Theme- Picture Gallery, Grade 11\n\n- This Project displays the great seven wonders of the world interactively depending on wherever you move your cursor. Enjoy!\n\n- https://www.codingal.com/competitions/iit-roorkee-kids-codathon/\n\n![image](https://user-images.githubusercontent.com/85481905/216280126-c0fba857-7037-4c98-a1fd-7e1a5fcc8004.png)\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Wonder-Gallery-23",
      "commitCount": 18,
      "latestCommitDate": "2023-02-11T15:21:50Z",
      "topics": [
        "website-development"
      ]
    },
    {
      "title": "Advanced-Java-Course",
      "description": "All my Java programs and files related to a udemy course",
      "readme": "# Advanced-Java\nThis is my progress of a advanced java course i bought on udemy. \nCourse link : https://www.udemy.com/course/java-in-depth-become-a-complete-java-engineer/\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Advanced-Java-Course",
      "commitCount": 4,
      "latestCommitDate": "2023-01-29T18:42:43Z",
      "topics": [
        "notes"
      ]
    },
    {
      "title": "teachnook-23-major-project",
      "description": "Major Project for internship. (Weather Website)",
      "readme": "# teachnook-23-major-project\nMajor Project for internship (Weather Website) . Website made using HTML, CSS and javascript along with openweather API, Bootstrap and ChartJS for fetching and plotting weather information.\n**[Time taken - 45mins]**\n\n![image](https://user-images.githubusercontent.com/85481905/215036139-a60744fe-3d5e-4db3-ba1f-94f41f77fef4.png)\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/teachnook-23-major-project",
      "commitCount": 4,
      "latestCommitDate": "2023-01-27T07:56:18Z",
      "topics": [
        "website-development"
      ]
    },
    {
      "title": "chess-engine-nn",
      "description": "Chess engine that uses neural network to decide on moves",
      "readme": "# Chess Engine with Neural Network\n\n![](demo.gif)\n\n## Motivation\n\nIt is inspired by TCEC Season 14 - Superfinal, where Leela was trying to fry Stockfish. [Stockfish](https://stockfishchess.org/) is chess engine that dominates among publicly available engines since 2014. Stockfish uses classic approach of chess engines, where everything is determined by algorithms built into engine. [Lc0 aka Leela Chess Zero](http://lczero.org/) is different, since it uses some sort of trainable neural network inside, as well as appealing human-like nickname. Mid-series, Leela managed to take the lead by 1-2 points, and there is a chance NN-powered chess engine will finish heuristics-based dominance (78/100 games played so far).\n\nI wanted to demonstrate my 7yr-old daughter that making these computer chess players are not as hard. We drew a diagram of playing and learning with NN and I started implementing it. The idea is to have practice and fun, make own mistakes, have discussions with daughter about planning and decision making. \n\nI understand there is ton of research done for centuries on all of the things I say below, I deliberately choose to make my own research for the fun of it.\n\n## Diagram\n![](diagram.png)\n\n## Neural Network Structure:\n![](model.png)\n\n\n## Journal\n\n### Feb 15, 2019\nFirst commits of the code.\nUsing own representation of chess board as 8 * 8 * 12 array of 1/0 values. 1/0 are used as most distinctive input for NN about piece presence at certain square. NN uses two hidden layers 64 nodes each. Output of NN is 8 * 8 array for \"from cell\" and 8 * 8 array for \"to cell\" scores. A piece of code is used to choose best move that a) is by chess rules; b) has best score of \"from\" multiplied by \"to\" output cells. Finally, board state is checked for game end conditions of checkmate or 50-move rule.\n\nTwo copies of NN are used, one plays as White and one as Black, playing versus each other. Game is recorded as PGN file, to be able to review it by human.\n\n### Feb 16, 2019\nDaughter decided that both NN copies urgently need girl-ish names. White is Lisa, black is Karen.\nI decided that writing chess rule checking and move validation is not the goal of this project. Threw away my code for it, in favor of [python-chess](https://python-chess.readthedocs.io/en/latest/) library. This shortened code x3 times.\n\nEngines are playing with each other, after each game they take move log and NN learns from it, assuming *all moves of lost game were bad, all moves of won game are good*, moves from draw are slightly bad (to motivate them search for better than draw).\n\nA web UI is added along with small API server to be able to play with White versus Karen, to try her. Lisa vs Karen are left to play with each other and learn overnight.\n\n### Feb 17, 2019\nThey played 12700 games overnight. Watching their game recordings shows they are doing a lot of dumb moves with same piece back and forth, making no progress unless they approach 3-fold or 50-move, when program forces to discard drawish move.\n\nIt seemed that it's deeply wrong to teach NN that any move from victorious game is good. I wanted to avoid this originally, but to make progress, we need to introduce some sort of *position evaluation*, so engine will learn to tell good moves from bad moves.\n\nWith daughter, we outlined some rules of evaluating position:\n1. Pieces have different value\n2. Capturing enemy piece is very good\n3. Attacking enemy piece is good\n4. Putting yourself under attack or removing defence from your piece is bad\n5. Increasing squares under our control is good\n6. Moving pawn forward is slightly good as it is the way to promote\n\nWe started with material balance, taking possible square control as value of the piece: \n - Pawn: 1\n - Knight: 8\n - Bishop: 13\n - Rook: 14\n - Queen: 27\n - King: 64 - special value to reflect it's precious\n\nSince we analyze game after all moves are done, we judge move by what was the consequence after opponent's move. We calculate sum of all our piece values, subtracting all enemy piece values, so we get *material balance*. Learning for ~3000 games shown that NNs definitely learn to capture pieces a lot, so the principle of evaluation works and reflects into NNs move score predictions. Material balance addresses #1, #2 and #3 of our evaluation rules.\n\nNext is to take *mobility* into account, to cover rules #3, #4, #5, #6 from above list. It is quite easy to implement, again we take state after opponent's move and we look now many moves can we make versus how many moves are there for opponent. Attacks are counted in a similar way, we assume that if piece is under attack, it adds *half* of its value to mobility score. This gives us \"mobility balance\".\n\nNow it gets tricky, since we need to combine material balance and mobility balance to get overall score for position. Current solution is to multiply material balance by 10 and sum with mobility balance. Gut feeling tells me it's not right, but let's see what NN will learn.\n\nBots are left to play overnight again.\n\n### 18 Feb, 2019\n\nThey've played ~32000 games. Quick game versus Karen shows that still NN miss obvious attacks and loses after human quickly. But this time it does a lot more captures, moves pieces more consistenly forward, does less \"dumb cyclic moves\".\n\nWatching games of Lisa vs Karen shows consistent pawn promotions and a lot of captures from both sides. Still most of obvious attacks, exchanges are missed. End game is completely dumb, falling into cyclic moves still.\n\nI did series of experiments with NN structure, looking if making it deeper or wider would help. But ~10000 games shows that neither 3-layer, nor 128-node NN does better. \n\nIt's time to summarize what was done over past days:\n  - building this kind of NN-driven test engine is super-easy\n  - the rules we put as criteria for learning do affect engine move making\n  - we should research/experiment with evaluation approach more\n  - we should experiment with NN structure more, maybe use LSTM instead of feed-forward\n  - re-learning from last N games might be better than incremental learning from very last game\n  - turning game representation to \"always one side\" and using exactly same NN for game might make it more robust and universal\n\n### 22 Feb, 2019\n\nDecided to make sing NN that will be able to decide for both sides, using board and move flip for black.\n\nNN structure changed to 6-layers decreasing from 768 to 128 nodes. This increased number of trainable parameters 100x times. It went back to learn games with a lot of dumb moves.\n\n### 23 Feb, 2019\n\nI tend to change my mind back. Actually, my program is just \"what is the best move from current position\" solver. It has no tree search part (intentional!), and it's wrong for NN to learn moves as good just because they are part of winning game. The score of move is mere measure of if it has changed position in our favor. At first, I want to get NN that will not do stupid mistakes.\n\n### 24 Feb, 2019\n\nLife of developer: was debugging code to understand why my accuracy metric of NN move quickly reaches 1.0 and stayes there. Found that I used wrong data type for numpy array, it was rounding my floats into ints. :facepalm:\n\n### 25 Feb, 2019\n\nStill nothing leads to learning good moves. Experimenting with various scores for moves. Cyclic moves demonstrate lack of depth. I revised the criteria for good move, made NN deeper and made it to learn by batches of 10 games. Let's see...\n\n### 4 Mar, 2019\n\n Thinking of crystallizing \"just next move predictor NN\" more. For that, I'll save dataset of all moves generated through learning, and will always learn from this big set. Previous results from multilayer NN were worse than 2-layer, will get back to 2-layer for now.\n \n I feel the trick to teach this variant of NN is to find correct board state representation, so NN will naturally learn.\n\n### 13 Mar, 2019\n\nI made many experiments yesterday, trying to figure out why NN still suggests dumb moves. One good thing I found is that feeding only good moves into learning process improves the NN accuracy in training, which actually makes sense. Still, the accuracy of \"move from\" output of NN is much better than \"move to\". In fact, we need \"move to\" as most valuable output.\n\n### 16 Mar, 2019\n\nFound that moves in training dataset were not unique. God knows how that was affecting learning process for NN... Also found out that material balance score were inverted for black, also affecting NN training scores. Good to find this, it gives me hope that it is possible to build NN like I want.\n\nBig problem is dataset balancing. Moves tend to use kings more, which leads NN to learn to move kings more. Trying to re-balance learning sample weights did not help.\n\nNo matter what I did, the best accuracy I get from NN is 0.5, which is equal to random. I will try to go back to principle of \"all moves from won game are good\" to research it again.\n\n### 30 Mar, 2019\n\nI see that \"noisy moves\" from draws affect NN learning in a bad way. Same for pointless moves along games with victories. Those moves don't lead to victory as much. So maybe if learning moves for NN would be chosen more selectively, it would help. Will try it in the morning (00:27 now)...\n\nWell, I experimented whole day and the result didn't change. Still stupid moves.\n\n### 14 Apr 2019\n\nOpenAI has beaten humans in Dota2. This world is doomed. Let's not postpone the unavoidable, I need to find my way to make this chess engine at least a bit smart.\nCurrent idea is to introduce \"intermediary goals\" of \"possible moves\", \"attacks\", \"threats\" as auxillary output layers. Maybe even \"defended pieces\". Let's see if NN can learn these simple position analysis concepts.\n\n... so, after day of coding, I've implemeted this approach. It is not clear if it is viable or not yet. The good outcome is that I've found a number of severe bugs and fixed them. Those bugs happened because of \"single NN for both White and Black\", a lot of confusion is there in the code to juggle sides. Cool thing is now I have visualization for model outputs. It will train overnight, though I don't expect outstanding results.\n\n### 15 Apr 2019\n\nAfter night of training, it has reached  slightly above 50% accuracy for choosing moves, but that's not much, since there are lots of repetitions and overall quality of moves is low. Still, it is better than what we had in the past.\n\n### 16 Apr 2019\n\nI'll experiment today's evening with two things: a) simplify the NN back to just inputs and outputs, with no aux outs; b) train with only victories and with score increasing from 0 to 1 the closer move to the end of game.\n\n... I need to understand how to avoid repetitions. The problem is that naturally NN falls into local optimums and suggest moves back and forth. One can't expect from this kind of non-temporal NN to work differently. So there should be a mechanism to avoid repetitions. Search tree is not an option, again. Just because whole idea of this engine is to avoid search tree.\n\n### 18 Apr 2019\n\nDoing several experiments:\n\n- Removing 3-fold repetition moves from learning data. Those moves are still present in the games, but learning input is nog garbaged by them.\n- Added \"eval\" output of NN, teaching it to evaluate win chance of the game. Game start with each player having 0.5 and then loser gets to 0.0 while winner gets to 1.0.\n- Playing back and forth with aux outputs of NN. They're still in question.\n\nAlso figured out my residual unit were increasing of size with each level. Reworked that to constant size of 8 * 8 * 12 (initial inputs)\n\n### 27 Jul 2019\n\nLet me take some rest from working for new startup. Starting to change the code with following thoughts:\n\n- Having 2 outputs 8x8 is somehow hard to train. Let's train 64*64=4096 possible moves output \n- Let's switch to [Chess960 aka Fischerandom](https://en.wikipedia.org/wiki/Chess960) for better training variety, since it is generalization over classical Chess\n- Still no good solution for 3-fold and 50-move avoiding\n- for now, model's structure will get back to 2-layer dense, to speed up experiments\n- reading [article](https://towardsdatascience.com/how-to-teach-an-ai-to-play-games-deep-reinforcement-learning-28f9b920440a) about Q-Learning, maybe the learning strategy should change... Also on same topic: https://www.askforgametask.com/tutorial/machine-learning-algorithm-flappy-bird/\n\nFound out that training data were using positions _after_ move made as source of training. Oh, my...\nFixing that seems helped learning a ton. Shame on me for overlooking this since the very beginning.\n\nI also made integration with UCI and now my NN plays versus Stockfish. Loses 100% of games, of course. Let it play with SF overnight...\n\n... it played 5000 games and dozens of re-trainings, but did not improve. I'll need to experiment more, still I see one important bug fixed.\n\n### 17 Aug 2019\n\nI have interesting thought. Chess board position is effectively an image. 8x8 pixels with 12 channels. So can we apply convolutional NN to analyze this image?\n\nI've build a NN structure that uses convolutional layers with additional route of data fed into dense layers, and now experimenting with hyperparams and training.\n\n... it played tens of thousands of games vs SF in \"retrain with last game\" mode, reached only 0.47 accuracy. But I already know there is one more problem with training data, black moves were not flipped properly. Figured it out in my head just before going to sleep. \n\n### 18 Aug 2019\n\nI fixed that issue with move flipping, will see how it affects the model. Looks like learning rate has improved a bit.\n\nAfter some training, I see that NN does not give good moves vs SF. Will train it more.\n\nAfter some more training, it has approached a bit over 0.5 accuracy, probably for the first time from all my attempts. It's all less about NN structure, it's mostly about stupid bugs in complicated code.\n\nStill after massive training done, accuracy is barely above 0.5. Will need to experiment with NN structure now, I guess.\n\n### 20 Aug 2019\n\nI decided to do one step back and start with simple 2-layer NN. And I see the accuracy close to 0.5, which means that we get better training without bugs in input data. \n\nAfter good amount of training I see that accuracy converges to 0.5, which does not mean any good state.\n\nOMG, _it makes meaningful captures for the first time_! That happened after I changed learning process to \"learn only from moves of winning side\", which I decided to do after reading some articles about how to deal with negative samples, softmax, and milticlass outputs. Though training accuracy is now around ~0.25, I like the impact on moves that I got.\n\nAs usual, I will leave it training versus SF for a night, maybe even for 48 hours. Let's see how it evolves. ... And after night of training it still converges into ~0.4 accuracy.\n\n### 3 Sep 2019\n\nHaving short break from primary job, I implemented more efficient way to represent possible moves output from NN: it includes only moves that are theoretically possible on board. This limits output size from 4096 into 1792 combinations, which serves well on dense type of layers.\n\nAlso, I changed structure of NN:\n - 2D-convolutional branches are made for kernel size 3, 4, 5, 6, 7, 8\n - concatenation of all conv branches comes into 2 aux outputs 8x8: attacked squares and defended squares\n - output from attacked and defended is concatenated with original position and fed into simple 2-layer Dense part. The assumption is that knowing piece positions, attacked squares and defended squares is sufficient to make \"not too dumb\" moves.\n - final move is decided on 1792 node output layer\n \n This net trains quite good now, I figured out loss, metrics and activations for aux and main outputs. Categorical accuracy of \"moves\" output is still below 0.5, but I see sample games with some attacks and captures, which is a good sign.\n \n Meanwhile, S16 - Division P is live on https://tcec.chessdom.com/, Leela plays versus Stockfish again...\n \n ### 10 Sep 2019\n \n Interesting thought: I should not use softmax on moves output layer. Just because possible moves are independent and score for them should be just independent sigmoid. Which also means that lost games can again be included in training set. There are some questions on training moves like this, since we only can pass single good/bad move as input. Ideal solution would be having own loss function.\n\n ### 18 Apr 2020\n\nThis time, superfinal between Lc0 and Stockfish happens again, season #17. We're in self-isolation because of COVID19, it's a good time to revise my chess AI project.\n\nI was thinking a lot about it, and many experiments were not merged into main branch, because they're all have failed.\nI'm ready to give up on my initial idea: build a NN that would \"say\" correct move. I realize that much more advanced\nreinforcement learning technique would be needed to achieve that.\n\nInstead, I will try to do a \"tree search with just 1 level\", to pick the right move. It assumes that central part is NN\nposition evaluator that is trusted. It's a sort of \"optimist\" approach where we believe that current eval is very smart\nand no deep tree search is needed.\n\n...\n\nAfter some training against SF, I see a problem of 1-ply depth: if a move actually leads to bad position (taking\nopponents' move into account), then all we have is choice from very bad moves.\n\n### 1 Dec 2021\n\nNepo plays with Carlsen for the Chess\nCrown (https://lichess.org/broadcast/world-chess-championship-2021/game-5/H8H4enOL). I want to try exercise on this\nproject again, using my 3060Ti and the approach of 1-level deep eval. The idea: for each position, we calculate score\nfor each possible next position. The top score is the move. I'm getting more modest, just want to get a model that makes not totally random moves, for example, does not miss capturing the queen for free.\n\nI put the current version to run overnight. In the morning it got to game #960, to try all possible starting positions.\nRan for 15 hours. It spends a lot of time making long endgames, so I'll add SyzygyDB checks for 3/4/5-piece endings.\n\nIt seems that with the approach of eval it is a _regression_ task.\n\nAt the end of this exercise, I see that quality of games does not grow much, even training against SF. Maybe the NN structure could be somehow improved to provide better position analysis capacity. I still miss good point on measuring the quality of games, what's the KPI. It's not NN loss, as far as I see.\n\n### 11 Nov 2022\n\nI wrote somewhere above 'bout COVID times... Well, I knew not that it can be that worse. Now I'm a voluntary exile and life won't be the same. Still, I watch TCEC season 23 superfinal and Leela loses it to SF miserably. \n\nMy mood is slightly better, and somehow I feel playing again with my chess program, to vent out my itchy engineering gland.\n\nI understand I would repeat something from above, but my thoughts are coming again to some statements:\n\n- I don't want the program to be the _search_ program that gives score to a position. Instead, I want it to be a prediction program, that would suggest the next move and _all_ logic should emerge inside NN, through its training.\n- The idea of input position => NN body => 1792 possible moves output\n- output layer should be `softmax`-activated\n- `categorical_crossentropy` should be a loss function\n\nI still believe that NN should learn at least basic valid moves rules. That will be the goal of the excercise.\n\n...\n\nChecked again all my code for some huge stupid misuse of NN - it all looks correct according to my expectations.\n\nObservations:\n- for body activation relu is good and linear is even better \n- rmsprop optimizer does not work for dense net\n- judging by decrease in number of invalid moves offered by NN, training of convolutional variant helps\n- `sigmoid` output layer seem to produce better probability of legal moves ",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/chess-engine-nn",
      "commitCount": 123,
      "latestCommitDate": "2023-01-06T19:03:41Z",
      "topics": [
        "artificial-intelligence"
      ]
    },
    {
      "title": "RDC",
      "description": "Full Stack Hospital Management Website for RDC",
      "readme": "# Hospital-Management\n\nFreelanced as a Frontend Developer for a hospital management website which is made using Reactjs, Tailwind and Sanity. Patients can register for their appointments and explore insights about the hospital. It has the functionality of SMS delivery and Real time Database Updates. (Backend Dev - @SaarD00)\n\n![image](https://user-images.githubusercontent.com/85481905/210169810-f02f98b6-86fe-4fb8-acb0-20657d6b31bd.png)\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/RDC",
      "commitCount": 61,
      "latestCommitDate": "2023-01-01T15:10:31Z",
      "topics": [
        "website-development"
      ]
    },
    {
      "title": "BlindChat",
      "description": "Anonymous Chatting app made using React, Tailwind and Firebase",
      "readme": "# Blind Chat - Anonymous Chatting app\n\nMade using Reactjs, Tailwind and Firebase. BlindChat is an anonymous chatting app that has the functionality of real time database update with messages and Google Authentication.\n\n![image](https://user-images.githubusercontent.com/85481905/210099656-e5f4b35c-9533-49ca-96cc-2ac2d4146b68.png)\n![image](https://user-images.githubusercontent.com/85481905/210099780-e5721990-013f-40e0-a2b3-4a6ec99256f0.png)\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/BlindChat",
      "commitCount": 7,
      "latestCommitDate": "2023-01-01T11:58:19Z",
      "topics": [
        "website-development"
      ]
    },
    {
      "title": "Listenify",
      "description": "Lofi Beats Website made using HTML CSS Javascript",
      "readme": "Lofi-Beats Music Player that has the functionality to play music from my Lofi-Beats.\n\n![preview](https://user-images.githubusercontent.com/85481905/199048055-1f6cce22-e2bd-499a-afc6-e524edfb5826.jpg)\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Listenify",
      "commitCount": 10,
      "latestCommitDate": "2022-10-31T15:38:19Z",
      "topics": [
        "website-development"
      ]
    },
    {
      "title": "Low-Level-Journey",
      "description": "All my C/C++/C# Course projects and notes",
      "readme": "",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Low-Level-Journey",
      "commitCount": 0,
      "latestCommitDate": null,
      "topics": []
    },
    {
      "title": "Merch-Website",
      "description": "Merch Website [my first publish ever]",
      "readme": "",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Merch-Website",
      "commitCount": 0,
      "latestCommitDate": null,
      "topics": []
    },
    {
      "title": "Wordle",
      "description": "Wordle Clone made using HTML, CSS and Javascript",
      "readme": "# Wordle\nWordle Clone made using using HTML, CSS and JavaScript.\n\n![preview](https://user-images.githubusercontent.com/85481905/199047484-e66d3669-ed64-4977-a3f1-4171b8afbb55.jpg)\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/Wordle",
      "commitCount": 6,
      "latestCommitDate": "2022-10-31T15:36:11Z",
      "topics": [
        "website-development"
      ]
    },
    {
      "title": "multitask-eye-disease-recognition",
      "description": "Multitask learning for eye disease recognition",
      "readme": "# Multitask Eye Disease Recognition\nMultitask learning for eye disease recognition. \n\nWork done in Microsoft AI Research. Published in ACPR'20. \n\nRecently, deep learning techniques have been widely used for medical image analysis. While there exists some work on deep learning for ophthalmology, there is little work on multi-disease predictions from retinal fundus images. Also, most of the work is based on small datasets. In this work, given a fundus image, we focus on three tasks related to eye disease prediction: (1) predicting one of the four broad disease categories – diabetic retinopathy, age-related macular degeneration, glaucoma, and melanoma, (2) predicting one of the 320 fine disease sub-categories, (3) generating a textual diagnosis. We model these three tasks under a multi-task learning setup using ResNet, a popular deep convolutional neural network architecture. Our experiments on a large dataset of 40658 images across 3502 patients provides ∼86% accuracy for task 1, ∼67% top-5 accuracy for task 2, and ∼32 BLEU for the diagnosis captioning task.\n\nLink to paper:- https://link.springer.com/chapter/10.1007/978-3-030-41299-9_57\n\n<b>Architecture Diagram</b>\n\n<img src ='arch.JPG' />\n\nRun the code with:- \n```\npython main.py\n```\n\nConfiguration can be modified in \n\n```\nconfig.gin\n```\n\n\n\n\n",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/multitask-eye-disease-recognition",
      "commitCount": 48,
      "latestCommitDate": "2022-10-19T12:00:45Z",
      "topics": [
        "artificial-intelligence"
      ]
    },
    {
      "title": "RecordScreen",
      "description": "Desktop Screen Recorder for simply recording screens made using Electron",
      "readme": "",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/RecordScreen",
      "commitCount": 0,
      "latestCommitDate": null,
      "topics": []
    },
    {
      "title": "SparkyAI-dash",
      "description": "Admin side dashboard for the @ sparkyai discord bot",
      "readme": "",
      "logoUrl": "https://opengraph.githubassets.com/1/ashworks1706/SparkyAI-dash",
      "commitCount": 0,
      "latestCommitDate": null,
      "topics": []
    }
  ],
  "nextFetch": "2025-01-11T11:19:57.234Z"
}